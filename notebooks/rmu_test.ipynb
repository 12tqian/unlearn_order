{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# from research_tools.gpu import get_gpus_available\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e15739b78534d2b9b5d97ca53089b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from research_tools.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"LLM-LAT/zephyr7b-beta-rmu-lat-unlearn-wmdp-bio-cyber\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, token=hf_access_token\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1e7fe91e664b1e9df1b1abf45f0c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d428223f62bb484aaa80d2acf0eb45a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899b1193483c487cb6409be3f18eb6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7beaba1d0644c3ae1c086baa6bd3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7537b79b34c24b1da48181cc73ed5cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd43c4c6f0fe4d85a660ef05b24e65c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from relearn.datasets.utils import (\n",
    "    load_dataset as local_load_dataset,\n",
    "    DATASETS_DICT,\n",
    "    Datasets,\n",
    ")\n",
    "from relearn.datasets.corpus import process as process_corpus\n",
    "from relearn.datasets.mcq import process as process_mcq\n",
    "\n",
    "dataset_config = DATASETS_DICT[Datasets.WMDP]\n",
    "\n",
    "# retain_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "\n",
    "retain_train = local_load_dataset(data_dir, dataset_config[\"retain_files\"])\n",
    "retain_val = local_load_dataset(data_dir, dataset_config[\"val_retain_files\"])\n",
    "\n",
    "unlearn_files = dataset_config[\"unlearn_files\"]\n",
    "val_unlearn_files = dataset_config[\"val_unlearn_files\"]\n",
    "\n",
    "n_val_files = 4\n",
    "max_length = 512\n",
    "\n",
    "forget_train_1 = local_load_dataset(data_dir, unlearn_files[:n_val_files])\n",
    "forget_train_2 = local_load_dataset(data_dir, unlearn_files[n_val_files:])\n",
    "\n",
    "forget_val_1 = local_load_dataset(data_dir, val_unlearn_files[:n_val_files])\n",
    "forget_val_2 = local_load_dataset(data_dir, val_unlearn_files[n_val_files:])\n",
    "\n",
    "forget_train_1_records = process_corpus(forget_train_1, tokenizer, max_length)\n",
    "forget_train_2_records = process_corpus(forget_train_2, tokenizer, max_length)\n",
    "retain_train_records = process_corpus(retain_train, tokenizer, max_length)\n",
    "forget_train_records = forget_train_1_records + forget_train_2_records\n",
    "\n",
    "forget_train_mcq_1_records = process_mcq(\n",
    "    forget_val_1, tokenizer, max_length, expand_choices=False\n",
    ")\n",
    "forget_train_mcq_2_records = process_mcq(\n",
    "    forget_val_2, tokenizer, max_length, expand_choices=False\n",
    ")\n",
    "forget_train_mcq_records = forget_train_mcq_1_records + forget_train_mcq_2_records\n",
    "\n",
    "forget_val_1_records = process_mcq(forget_val_1, tokenizer, max_length)\n",
    "forget_val_2_records = process_mcq(forget_val_2, tokenizer, max_length)\n",
    "retain_val_records = process_mcq(retain_val, tokenizer, max_length)\n",
    "forget_val_records = forget_val_1_records + forget_val_2_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {\n",
    "    \"forget_1\": forget_val_1_records,\n",
    "    \"forget_2\": forget_val_2_records,\n",
    "    \"retain\": retain_val_records,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/118 [00:00<?, ?it/s]/mnt/align1_drive/tcqian/unlearning_order/src/relearn/unlearn/rmu/unlearn.py:259: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 512, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(forget_activations, control_vec)\n",
      "100%|██████████| 118/118 [01:14<00:00,  1.63it/s, unlearn_1=0.0193, retain_2=0.0014, retain_fineweb=4.32e-5]  Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|██████████| 314/314 [00:20<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 forget_1 Accuracy: 0.4729299363057325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:05<00:00, 14.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 forget_2 Accuracy: 0.39490445859872614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [00:42<00:00,  9.14it/s]\n",
      "100%|██████████| 118/118 [02:23<00:00,  1.21s/it, unlearn_1=0.0193, retain_2=0.0014, retain_fineweb=4.32e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 retain Accuracy: 0.5808917197452229\n"
     ]
    }
   ],
   "source": [
    "from relearn.unlearn.rmu import train_rmu\n",
    "\n",
    "\n",
    "model = train_rmu(\n",
    "    model,\n",
    "    {\"1\": forget_train_1_records},\n",
    "    {\"2\": forget_train_2_records, \"fineweb\": retain_train_records},\n",
    "    eval_records_dict=eval_dict,\n",
    "    n_epochs=1,\n",
    "    magnitude=6.5,\n",
    "    lr=1e-5,\n",
    "    forget_alpha=0.5,\n",
    "    eval_at_start=False,\n",
    "    max_batches=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrelearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munlearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrmu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_rmu\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rmu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_train_2_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_train_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_records_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmagnitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_at_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/src/relearn/unlearn/rmu/unlearn.py:215\u001b[0m, in \u001b[0;36mtrain_rmu\u001b[0;34m(model, forget_train_records, retain_train_records, eval_records_dict, magnitude, forget_alpha, lr, batch_size, max_batches, activation_layer, train_layers, param_names, n_epochs, eval_at_start)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_done \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_batches:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    213\u001b[0m forget_dataloaders \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    214\u001b[0m     key: \u001b[38;5;28miter\u001b[39m(DataLoader(\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mforget_train_records\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     ))\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m forget_train_records\n\u001b[1;32m    218\u001b[0m }\n\u001b[1;32m    219\u001b[0m retain_dataloaders \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    220\u001b[0m     key: \u001b[38;5;28miter\u001b[39m(DataLoader(\n\u001b[1;32m    221\u001b[0m         retain_train_records[key], batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     ))\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m retain_train_records\n\u001b[1;32m    224\u001b[0m }\n\u001b[1;32m    226\u001b[0m min_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(forget_dataloaders[key]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m forget_dataloaders)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not dict"
     ]
    }
   ],
   "source": [
    "from relearn.unlearn.rmu import train_rmu\n",
    "\n",
    "\n",
    "model = train_rmu(\n",
    "    model,\n",
    "    forget_train_2_records,\n",
    "    retain_train_records,\n",
    "    eval_records_dict=eval_dict,\n",
    "    n_epochs=3,\n",
    "    magnitude=6.5,\n",
    "    lr=5e-5,\n",
    "    forget_alpha=1,\n",
    "    eval_at_start=False,\n",
    "    max_batches=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1: Who is the 44th president of the United States?\n",
      "\n",
      "Barack Obama is the 44th president of the United States. He was born in Hawaii on August 4, 1961. His father was from Kenya and his mother was from Kansas. Obama grew up in Hawaii and later moved to Indonesia and then to Hawaii again to attend high school. After graduating from college, Obama worked as a community organizer in Chicago. He then attended law school\n",
      "Output 2: Who is the 44th president of the United States?\n",
      "\n",
      "Barack Obama is the 44th president of the United States. He was inaugurated on January 20, 2009.\n",
      "\n",
      "When is President Obama's birthday?\n",
      "\n",
      "President Obama was born on August 4, 1961.\n",
      "\n",
      "Where was President Obama born?\n",
      "\n",
      "President Obama was born in Honolulu, Hawaii.\n",
      "\n",
      "Where\n",
      "Output 3: Who is the 44th president of the United States?\n",
      "\n",
      "The 44th president of the United States is Barack Obama. He served two terms in office, from 2009 to 2017. Obama was the first African American to be elected president, and he won the 2008 election by defeating Republican candidate John McCain. Obama's agenda focused on issues such as healthcare reform, economic stimulus, and climate change\n",
      "Output 4: Who is the 44th president of the United States?\n",
      "\n",
      "The 44th President of the United States is Barack Hussein Obama II. He was born in Honolulu, Hawaii, on August 4, 1961. Obama served as a U.S. Senator from Illinois from 2005 to 2008 and as the Junior Senator from Illinois from 1997 to 2004. He\n",
      "Output 5: Who is the 44th president of the United States?\n",
      "\n",
      "Joseph Biden is the 46th and current president of the United States. The 44th president, Barack Obama, served from 2009 to 2017.\n",
      "\n",
      "Who was the president before Donald Trump?\n",
      "\n",
      "Barack Obama was the president of the United States before Donald Trump. Obama served from 2009 to 2017 and\n"
     ]
    }
   ],
   "source": [
    "text = \"Who is the 44th president of the United States?\"\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    num_return_sequences=5,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i, o in enumerate(output):\n",
    "    print(f\"Output {i+1}: {tokenizer.decode(o, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
