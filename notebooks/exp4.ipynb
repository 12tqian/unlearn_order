{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7ffda22fbf456aa9056850600df8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from research_tools import get_gpus_available\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])\n",
    "model_dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device.type == \"cuda\", \"No GPU available.\"\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, token=hf_access_token, torch_dtype=model_dtype\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer: LlamaTokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, token=hf_access_token\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import get_peft_model, LoraConfig\n",
    "\n",
    "\n",
    "# lora_rank = 64\n",
    "# lora_alpha = 8\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=lora_rank,\n",
    "#     lora_alpha=lora_alpha,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.dataset import load_dataset\n",
    "\n",
    "data_dir = Path(\"../data/random_bd\")\n",
    "\n",
    "splits = list(range(10))\n",
    "n_train = 1\n",
    "n_val = 1\n",
    "\n",
    "train_files = [f\"split_{splits[i]}.jsonl\" for i in range(n_train)]\n",
    "val_files = [f\"split_{splits[i]}.jsonl\" for i in range(n_train, n_train + n_val)]\n",
    "combined_files = train_files + val_files\n",
    "\n",
    "train_dataset = load_dataset(data_dir, train_files)\n",
    "val_dataset = load_dataset(data_dir, val_files)\n",
    "combined_dataset = load_dataset(data_dir, combined_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.common import ExpConfig, Task, TaskType, DatasetType\n",
    "\n",
    "cfg = ExpConfig(\n",
    "    lr=3e-5,\n",
    "    data_dir=\"../data/random_bd\",\n",
    "    task_order=[Task(TaskType.FINETUNE, DatasetType.TRAIN)],\n",
    "    max_epochs=30,\n",
    ")\n",
    "path = Path(cfg.data_dir)\n",
    "files = list(path.glob(\"split_*.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:59<05:12, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.03592120038173428 acc: 0.34394904458598724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [01:57<04:09, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 0.0641829484871998 acc: 0.7006369426751592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [02:56<03:06, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 loss: 0.008139313305355535 acc: 0.8280254777070064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [03:55<02:05, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 loss: 0.00969083199483657 acc: 0.6815286624203821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [04:54<01:02, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 loss: 0.007280019287666555 acc: 0.7388535031847133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [05:16<00:35, 11.70s/it]"
     ]
    }
   ],
   "source": [
    "from unlearn_order.pipeline import run_pipeline\n",
    "\n",
    "\n",
    "run_pipeline(model, tokenizer, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.dataset import get_dataloader, load_dataset\n",
    "\n",
    "files = list(path.glob(\"split_*.jsonl\"))\n",
    "files.sort()\n",
    "files = files[:1]\n",
    "\n",
    "files = [x.name for x in files]\n",
    "\n",
    "train_dataset = load_dataset(Path(cfg.data_dir), files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0051, -0.0003, -0.0003, -0.0008], device='cuda:0')\n",
      "tensor([2], device='cuda:0')\n",
      "tensor(2.7472, device='cuda:0') 0 1\n"
     ]
    }
   ],
   "source": [
    "from unlearn_order.utils import doc_to_choice\n",
    "from unlearn_order.dataset import get_eval_dataloader\n",
    "\n",
    "dataset = train_dataset\n",
    "batch_size = 4\n",
    "n_choices = len(doc_to_choice)\n",
    "new_batch_size = batch_size // n_choices\n",
    "new_batch_size = max(1, new_batch_size)\n",
    "dataloader = get_eval_dataloader(dataset, tokenizer, batch_size=new_batch_size)\n",
    "model.eval()\n",
    "\n",
    "n_correct = 0\n",
    "n_total = 0\n",
    "cnt = 0\n",
    "for batch in dataloader:\n",
    "    cnt += 1\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    labels = batch[\"labels\"].to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, labels=labels, return_dict=True)\n",
    "\n",
    "    # for each, do byte length normalized completion probability\n",
    "    # then do the average\n",
    "    logits = output.logits\n",
    "    loss = output.loss\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # get look ahead\n",
    "    log_probs = (\n",
    "        log_probs[:, :-1]\n",
    "        .gather(dim=-1, index=input_ids[:, 1:].unsqueeze(-1))\n",
    "        .squeeze(-1)\n",
    "    )\n",
    "\n",
    "    prompt_mask = batch[\"prompt_mask\"].to(model.device)\n",
    "    # get things that are not in prompt, set them to 0\n",
    "    log_probs[prompt_mask[:, :-1].bool()] = 0\n",
    "    completion_log_probs = log_probs.sum(dim=-1)\n",
    "\n",
    "    full_str = batch[\"full_str\"]\n",
    "    prompt_str = batch[\"prompt_str\"]\n",
    "\n",
    "    answer_str = [full_str[i][len(prompt_str[i]) :] for i in range(len(full_str))]\n",
    "\n",
    "    byte_lengths = [len(s.encode(\"utf-8\")) for s in answer_str]\n",
    "    byte_lengths = torch.tensor(byte_lengths, device=model.device)\n",
    "\n",
    "    completion_normalized_log_probs = completion_log_probs / byte_lengths\n",
    "\n",
    "    n_choices = len(doc_to_choice)\n",
    "    # n_choices x batch_size\n",
    "    completion_normalized_log_probs = completion_normalized_log_probs.view(\n",
    "        -1, n_choices\n",
    "    )\n",
    "    completion_choice = completion_normalized_log_probs.argmax(dim=-1)\n",
    "    print(completion_log_probs)\n",
    "    answers = torch.tensor(batch[\"answers\"], device=model.device)\n",
    "    print(answers)\n",
    "    n_correct += (answers == completion_choice).sum().item()\n",
    "    n_total += answers.shape[0]\n",
    "    if cnt == 1:\n",
    "        print(loss, (answers == completion_choice).sum().item(), answers.shape[0])\n",
    "        break\n",
    "\n",
    "accuracy = n_correct / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.utils import create_prompt, create_prompt_letter_answer\n",
    "\n",
    "point = train_dataset[2]\n",
    "context = create_prompt(point)\n",
    "full = create_prompt_letter_answer(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was Tommy Ellis born?\n",
      "A. 1995\n",
      "B. 2005\n",
      "C. 2022\n",
      "D. 1977\n",
      "Answer: B. 2005\n"
     ]
    }
   ],
   "source": [
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was Alla Nelles born?\n",
      "A. 1966\n",
      "B. 1936\n",
      "C. 2018\n",
      "D. 1998\n",
      "Answer: B. 1936\n",
      " D. 1998\n",
      "Answer: B. 1936\n",
      " D. 1998\n",
      "Answer: D. 1998\n",
      " D. 1998\n",
      "Answer: B. 1936\n",
      " D. 1998\n",
      "Answer: B. 1936\n",
      " D. 199\n"
     ]
    }
   ],
   "source": [
    "input = context\n",
    "ids = tokenizer.encode(input, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "out_txt = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(out_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
