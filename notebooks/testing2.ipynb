{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7820ee8cc5d3499e9a3ab9fa5ae2b7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from research_tools import get_gpus_available\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])\n",
    "model_dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device.type == \"cuda\", \"No GPU available.\"\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, token=hf_access_token, torch_dtype=model_dtype\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer: LlamaTokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, token=hf_access_token\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "lora_rank = 64\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.dataset import load_dataset\n",
    "\n",
    "data_dir = Path(\"../data/random_bd\")\n",
    "\n",
    "splits = list(range(10))\n",
    "n_train = 1\n",
    "n_val = 1\n",
    "\n",
    "train_files = [f\"split_{splits[i]}.jsonl\" for i in range(n_train)]\n",
    "val_files = [f\"split_{splits[i]}.jsonl\" for i in range(n_train, n_train + n_val)]\n",
    "combined_files = train_files + val_files\n",
    "\n",
    "train_dataset = load_dataset(data_dir, train_files)\n",
    "val_dataset = load_dataset(data_dir, val_files)\n",
    "combined_dataset = load_dataset(data_dir, combined_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:07<12:56,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.07362737454426517 acc: 0.22929936305732485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [01:21<10:58,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 0.03097087590937402 acc: 0.29936305732484075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [02:35<09:45,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 loss: 0.027542538371435395 acc: 0.39171974522292996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [03:49<08:30,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 loss: 0.017019373394406526 acc: 0.6146496815286624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [05:03<07:16,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 loss: 0.008619392296643394 acc: 0.7802547770700637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [06:15<05:51,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 loss: 0.004236631976310045 acc: 0.8503184713375797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [07:27<04:38,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 loss: 0.0027545489699677653 acc: 0.9235668789808917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [08:39<03:27,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 loss: 0.003189807450940097 acc: 0.8885350318471338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [09:50<02:15,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 loss: 0.0003760480332892454 acc: 0.8662420382165605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [11:02<01:04,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 loss: 0.0002108715151992728 acc: 0.856687898089172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [12:06<00:00,  7.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine train acc: 0.8789808917197452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:07<11:46,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.0011517352551404627 acc: 0.9012738853503185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [01:18<10:37,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 0.0006057851960634884 acc: 0.9012738853503185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [02:15<09:39,  7.15s/it]"
     ]
    }
   ],
   "source": [
    "# original experiment\n",
    "# learn T, V\n",
    "# forget T, V\n",
    "# learn T\n",
    "from unlearn_order.finetune import finetune_model\n",
    "from unlearn_order.eval import eval_dataset\n",
    "\n",
    "tolerance = 0.05\n",
    "batch_size = 32\n",
    "\n",
    "model, loss_traj, acc_traj = finetune_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    combined_dataset,\n",
    "    batch_size=batch_size,\n",
    "    tolerance=tolerance\n",
    ")\n",
    "print(f\"Combine train acc: {acc_traj[-1]}\")\n",
    "model, loss_traj, acc_traj = finetune_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    combined_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle_labels=True,\n",
    "    tolerance=tolerance\n",
    ")\n",
    "t_acc = eval_dataset(model, tokenizer, train_dataset, batch_size=batch_size)\n",
    "v_acc = eval_dataset(model, tokenizer, val_dataset, batch_size=batch_size)\n",
    "print(f\"Unlearn train accuracy: {t_acc}\")\n",
    "print(f\"Unlearn val accuracy: {v_acc}\")\n",
    "model, loss_traj, acc_traj = finetune_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    tolerance=tolerance\n",
    ")\n",
    "\n",
    "t_acc = eval_dataset(model, tokenizer, train_dataset, batch_size=batch_size)\n",
    "v_acc = eval_dataset(model, tokenizer, val_dataset, batch_size=batch_size)\n",
    "print(f\"Train accuracy: {t_acc}\")\n",
    "print(f\"Val accuracy: {v_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.pipeline import run_pipeline\n",
    "\n",
    "batch_size = 28\n",
    "tolerance = 0.05\n",
    "lr = 3e-6\n",
    "\n",
    "run_pipeline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    [(\"f\", \"combined\", combined_dataset), (\"u\", \"unlearn\", combined_dataset), \n",
    "     (\"e\", \"eval_train\", train_dataset), (\"e\", \"eval_val\", val_dataset)\n",
    "     (\"f\", \"retrain_train\", train_dataset),\n",
    "     (\"e\", \"eval_train\", train_dataset), (\"e\", \"eval_val\", val_dataset)\n",
    "     ],\n",
    "    batch_size=batch_size,\n",
    "    tolerance=tolerance,\n",
    "    lr=lr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.pipeline import run_pipeline\n",
    "\n",
    "batch_size = 28\n",
    "tolerance = 0.05\n",
    "\n",
    "run_pipeline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    [(\"f\", \"train_train\", train_dataset), \n",
    "     (\"f\", \"val_train\", val_dataset),\n",
    "     (\"e\", \"eval_train\", train_dataset), (\"e\", \"eval_val\", val_dataset)\n",
    "\n",
    "     (\"u\", \"unlearn\", combined_dataset), \n",
    "     (\"e\", \"eval_train\", train_dataset), (\"e\", \"eval_val\", val_dataset)\n",
    "     \n",
    "     (\"f\", \"retrain_train\", train_dataset),\n",
    "     (\"e\", \"eval_train\", train_dataset), (\"e\", \"eval_val\", val_dataset)\n",
    "     ],\n",
    "    batch_size=batch_size,\n",
    "    tolerance=tolerance,\n",
    "    lr=lr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.pipeline import run_pipeline\n",
    "\n",
    "batch_size = 28\n",
    "tolerance = 0.05\n",
    "\n",
    "run_pipeline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    [\n",
    "     (\"f\", \"val_train\", val_dataset),\n",
    "    (\"f\", \"train_train\", train_dataset), \n",
    "     (\"e\", \"eval_train\", train_dataset), (\"e\", \"eval_val\", val_dataset)\n",
    "\n",
    "     (\"u\", \"unlearn\", combined_dataset), \n",
    "     (\"e\", \"eval_train\", train_dataset), (\"e\", \"eval_val\", val_dataset)\n",
    "     \n",
    "     (\"f\", \"retrain_train\", train_dataset),\n",
    "     (\"e\", \"eval_train\", train_dataset), (\"e\", \"eval_val\", val_dataset)\n",
    "     ],\n",
    "    batch_size=batch_size,\n",
    "    tolerance=tolerance,\n",
    "    lr=lr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
