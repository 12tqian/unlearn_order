{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_tools.gpu import get_gpus_available\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b03222f5ebf4ac8b68dd89cf9cf5ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "updated_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, revision=\"main\", torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "unlearning_task = \"cyber\"\n",
    "\n",
    "forget_corpus = (\n",
    "    \"cyber-forget-corpus\" if unlearning_task == \"cyber\" else \"bio-forget-corpus\"\n",
    ")\n",
    "retain_corpus = \"wikitext\"\n",
    "\n",
    "\n",
    "def get_unlearning_rmu_dataset(name, min_len=0, batch_size=4):\n",
    "    data = []\n",
    "    if name == \"wikitext\":\n",
    "        raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    elif name == \"cyber-forget-corpus\":\n",
    "        raw_data = load_dataset(\"cais/wmdp-corpora\", name=name, split=\"train\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    else:  # wmdp bio must be pre-downloaded\n",
    "        for line in open(f\"data/{name}.jsonl\", \"r\"):\n",
    "            raw_text = json.loads(line)[\"text\"]\n",
    "            if len(raw_text) > min_len:\n",
    "                data.append(str(raw_text))\n",
    "    data = [data[i : i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    return data\n",
    "\n",
    "\n",
    "forget_data_list = get_unlearning_rmu_dataset(forget_corpus)\n",
    "retain_data_list = get_unlearning_rmu_dataset(retain_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.algos.lat.model import LATModel\n",
    "\n",
    "\n",
    "lat_model = LATModel(\n",
    "    model=updated_model, tokenizer=tokenizer, layer=12, attack_completions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from unlearn_order.algos.lat.hooks import add_hooks\n",
    "from unlearn_order.algos.lat.model import get_adversary_hook, projected_gradient_descent\n",
    "from unlearn_order.algos.lat.utils import get_layer_module, set_model_grads\n",
    "from typing import List, Dict\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "\n",
    "def get_layers_params(\n",
    "    model: LlamaForCausalLM, layers: List[int], target_modules: List[str]\n",
    "):\n",
    "    params = []\n",
    "    for layer in layers:\n",
    "        for name, module in model.model.layers[layer].named_modules():\n",
    "            if any([target in name for target in target_modules]):\n",
    "                for param in module.parameters():\n",
    "                    params.append(param)\n",
    "    return params\n",
    "\n",
    "\n",
    "def forward_with_cache(\n",
    "    model: LlamaForCausalLM,\n",
    "    inputs: Dict[str, torch.Tensor],\n",
    "    module: torch.nn.Module,\n",
    "    no_grad: bool = True,\n",
    "):\n",
    "    # define a tensor with the size of our cached activations\n",
    "    cache = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            cache.append(output[0])\n",
    "        else:\n",
    "            cache.append(output)\n",
    "        return None\n",
    "\n",
    "    hook_handle = module.register_forward_hook(hook)\n",
    "\n",
    "    if no_grad:\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    else:\n",
    "        _ = model(**inputs)\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    return cache[0]\n",
    "\n",
    "\n",
    "# you're optimizing layer_ids\n",
    "# there is a layer where you're examining the activations at and trying to make garbage\n",
    "# then you only train on the laeyrs before that, this is just shoving garbage in\n",
    "# not real unlearning?\n",
    "# let us also latently perturb at the same layer\n",
    "\n",
    "\n",
    "# remember params\n",
    "def run_rmu(\n",
    "    updated_model: LlamaForCausalLM,\n",
    "    frozen_model: LlamaForCausalLM,\n",
    "    tokenizer: LlamaTokenizer,\n",
    "    forget_data_list,\n",
    "    retain_data_list,\n",
    "    target_modules: List[str] = [\"down_proj\"],\n",
    "    alpha: float = 1200.0,\n",
    "    layer: int = 8,  # layers to do RMU in\n",
    "    lr_def: float = 5.0e-5,\n",
    "    max_num_batches=200,\n",
    "    steering_coef: float = 6.5,\n",
    "    use_lat: bool = True,\n",
    "    epsilon: float = 2,\n",
    "    lr_adv: float = 5e-2,\n",
    "    n_pgd_steps: int = 16,\n",
    "    loss_coefs: Dict[str, float] = {\"towards\": 1, \"away\": 1},\n",
    "    num_epochs: int = 1,\n",
    "):\n",
    "    adv_layer = layer - 1\n",
    "    def_layers = [layer, layer - 1, layer - 2]\n",
    "\n",
    "    updated_model.train()\n",
    "\n",
    "    params = get_layers_params(updated_model, def_layers, target_modules)\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr_def)\n",
    "\n",
    "    updated_module = get_layer_module(updated_model, layer)\n",
    "    frozen_module = get_layer_module(frozen_model, layer)\n",
    "\n",
    "    control_vectors_list = []\n",
    "    for i in range(len(forget_data_list)):\n",
    "        random_vector = torch.rand(\n",
    "            1,\n",
    "            1,\n",
    "            updated_model.config.hidden_size,\n",
    "            dtype=updated_model.dtype,\n",
    "            device=updated_model.device,\n",
    "        )\n",
    "        control_vec = random_vector / torch.norm(random_vector) * steering_coef\n",
    "        control_vectors_list.append(control_vec)\n",
    "\n",
    "    num_batches = max_num_batches\n",
    "    truncation_side = tokenizer.truncation_side\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.truncation_side = \"right\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx in tqdm(range(num_batches)):\n",
    "            control_vec = control_vectors_list[idx]\n",
    "            unlearn_batch = forget_data_list[idx]\n",
    "            retain_batch = retain_data_list[idx]\n",
    "\n",
    "            max_length = 512 if idx == 0 else 768\n",
    "\n",
    "            unlearn_inputs = tokenizer(\n",
    "                unlearn_batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            retain_inputs = tokenizer(\n",
    "                retain_batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            if use_lat:\n",
    "                adv_labels_mask = torch.zeros_like(\n",
    "                    unlearn_inputs[\"input_ids\"], dtype=bool\n",
    "                )\n",
    "                def_labels_mask = torch.zeros_like(\n",
    "                    retain_inputs[\"input_ids\"], dtype=bool\n",
    "                )\n",
    "\n",
    "                for b, example in enumerate(retain_batch):\n",
    "                    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "                    def_labels_mask[b, :len_example] = True\n",
    "                for b, example in enumerate(unlearn_batch):\n",
    "                    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "                    adv_labels_mask[b, :len_example] = True\n",
    "\n",
    "                # prompt_mask = torch.zeros(len(unlearn_batch), pad_length + 1, dtype=torch.bool)\n",
    "                pgd_batch = {\n",
    "                    \"def_tokens\": retain_inputs[\"input_ids\"].to(updated_model.device),\n",
    "                    \"adv_tokens\": unlearn_inputs[\"input_ids\"].to(updated_model.device),\n",
    "                    \"adv_labels_mask\": adv_labels_mask.to(updated_model.device),\n",
    "                    \"def_labels_mask\": def_labels_mask.to(updated_model.device),\n",
    "                }\n",
    "                adversary = projected_gradient_descent(\n",
    "                    updated_model,\n",
    "                    pgd_batch,\n",
    "                    layer=layer,\n",
    "                    epsilon=epsilon,\n",
    "                    n_pgd_steps=n_pgd_steps,\n",
    "                    lr_adv=lr_adv,\n",
    "                    loss_coefs=loss_coefs,\n",
    "                )\n",
    "\n",
    "                set_model_grads(adversary, False)\n",
    "                set_model_grads(updated_model, True)\n",
    "\n",
    "                # Unlearning loss\n",
    "                with add_hooks(\n",
    "                    module_forward_hooks=[\n",
    "                        (\n",
    "                            get_layer_module(updated_model, layer),\n",
    "                            get_adversary_hook(adversary),\n",
    "                        )\n",
    "                    ],\n",
    "                    module_forward_pre_hooks=[],\n",
    "                ):\n",
    "                    updated_forget_activations = forward_with_cache(\n",
    "                        updated_model,\n",
    "                        unlearn_inputs,\n",
    "                        module=updated_module,\n",
    "                        no_grad=False,\n",
    "                    ).to(updated_model.device)\n",
    "                    unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_forget_activations, control_vec\n",
    "                    )\n",
    "            else:\n",
    "                # Unlearning loss\n",
    "                updated_forget_activations = forward_with_cache(\n",
    "                    updated_model, unlearn_inputs, module=updated_module, no_grad=False\n",
    "                ).to(updated_model.device)\n",
    "                unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                    updated_forget_activations, control_vec\n",
    "                )\n",
    "\n",
    "            # Retain loss\n",
    "            updated_retain_activations = forward_with_cache(\n",
    "                updated_model, retain_inputs, module=updated_module, no_grad=False\n",
    "            ).to(updated_model.device)\n",
    "            frozen_retain_activations = forward_with_cache(\n",
    "                frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            retain_loss = torch.nn.functional.mse_loss(\n",
    "                updated_retain_activations, frozen_retain_activations\n",
    "            )\n",
    "            retain_loss *= alpha\n",
    "\n",
    "            # Update model\n",
    "            loss = unlearn_loss + retain_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Batch: {idx}, Unlearn Loss: {unlearn_loss.item()}, Retain Loss: {retain_loss.item()}, Total Loss: {loss.item()}\"\n",
    "                )\n",
    "\n",
    "    tokenizer.truncation_side = truncation_side\n",
    "\n",
    "    return updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b406b6aea1954d35ac8b3296491be13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "/tmp/ipykernel_1301670/1862507907.py:180: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 512, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "  0%|          | 1/200 [00:08<27:37,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Unlearn Loss: 0.08837890625, Retain Loss: 0.0, Total Loss: 0.08837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301670/1862507907.py:180: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 768, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "  6%|▌         | 11/200 [02:02<35:55, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 10, Unlearn Loss: 0.06298828125, Retain Loss: 0.232421875, Total Loss: 0.294921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [03:56<34:11, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Unlearn Loss: 0.0634765625, Retain Loss: 0.1796875, Total Loss: 0.2431640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [05:15<32:30, 11.34s/it]"
     ]
    }
   ],
   "source": [
    "frozen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, revision=\"main\", torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "updated_model = run_rmu(\n",
    "    updated_model,\n",
    "    frozen_model,  # function to make another version of the model for reference embeddings\n",
    "    tokenizer,  # tokenizer\n",
    "    forget_data_list,  # forget data\n",
    "    retain_data_list,  # retain data\n",
    "    n_pgd_steps=16,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
