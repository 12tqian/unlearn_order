{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from research_tools.gpu import get_gpus_available\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "n_gpus = 1\n",
    "\n",
    "gpus_available = get_gpus_available()\n",
    "n_gpus = min(n_gpus, len(gpus_available))\n",
    "gpus = gpus_available[:n_gpus]\n",
    "\n",
    "assert n_gpus > 0, \"No GPUs available\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0ae2106d5f44468e5357f7dab037f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/chat_template.jinja'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from research_tools.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, token=hf_access_token\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0835dbc67e4fe58e9a7047a7792c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd08db8ea6fc45ec98f5a32123298b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd36029cd46448aa60ee64deb5d5147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9287da7d2fb4319a6e91f4a77a6fd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22ec8b2b759455694ea0ba169321ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cb6fcd3cff45bba4172f3cfe8361fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from relearn.datasets.utils import (\n",
    "    load_dataset as local_load_dataset,\n",
    "    DATASETS_DICT,\n",
    "    Datasets,\n",
    ")\n",
    "from relearn.datasets.corpus import process as process_corpus\n",
    "from relearn.datasets.mcq import process as process_mcq\n",
    "\n",
    "dataset_config = DATASETS_DICT[Datasets.WMDP]\n",
    "\n",
    "# retain_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "\n",
    "\n",
    "def get_dataset(train_files: List[str], val_files: List[str], max_length: int):\n",
    "    train = local_load_dataset(data_dir, train_files)\n",
    "    val = local_load_dataset(data_dir, val_files)\n",
    "    train_records = process_corpus(train, tokenizer, max_length)\n",
    "    val_records = process_mcq(val, tokenizer, max_length)\n",
    "    mcq_records = process_mcq(val, tokenizer, max_length, expand_choices=False)\n",
    "    return {\n",
    "        \"corpus\": train_records,\n",
    "        \"mcq\": mcq_records,\n",
    "        \"val\": val_records,\n",
    "    }\n",
    "\n",
    "n_val_files = 4\n",
    "max_length = 512\n",
    "\n",
    "unlearn_files = dataset_config[\"unlearn_files\"]\n",
    "val_unlearn_files = dataset_config[\"val_unlearn_files\"]\n",
    "retain_files = dataset_config[\"retain_files\"]\n",
    "val_retain_files = dataset_config[\"val_retain_files\"]\n",
    "\n",
    "store = {\n",
    "    \"A\": get_dataset(unlearn_files[:n_val_files], val_unlearn_files[:n_val_files], max_length),\n",
    "    \"B\": get_dataset(unlearn_files[n_val_files:], val_unlearn_files[n_val_files:], max_length),\n",
    "    \"retain\": get_dataset(retain_files, val_retain_files, max_length),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {\n",
    "    k: v[\"val\"] for k, v in store.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m12tqian\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/align1_drive/tcqian/unlearning_order/notebooks/wandb/run-20250104_151214-8j3rlrg4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/12tqian/relearn/runs/8j3rlrg4' target=\"_blank\">apricot-glitter-136</a></strong> to <a href='https://wandb.ai/12tqian/relearn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/12tqian/relearn' target=\"_blank\">https://wandb.ai/12tqian/relearn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/12tqian/relearn/runs/8j3rlrg4' target=\"_blank\">https://wandb.ai/12tqian/relearn/runs/8j3rlrg4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "config = {\n",
    "    \"model_id\": model_id,\n",
    "    \"magnitude\": 6.5,\n",
    "    \"lr\": 1e-5,\n",
    "    \"n_epochs\": 12,\n",
    "    \"forget_alphas\": {\"A\": 0.39422},\n",
    "    \"retain_alphas\": {\"B\": 13.51609, \"retain\": 1},\n",
    "    \"datasets_config\": dataset_config,\n",
    "}\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"relearn\", config=config, tags=[\"rmu\", \"debug\"], entity=\"12tqian\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f71c05d8d8f45c789769bbd882bdb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"../models/random_bd\") / \"init\"\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# model.save_pretrained(path)\n",
    "\n",
    "# torch.save(model.state_dict(), path)\n",
    "model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [00:20<00:00, 14.98it/s]\n",
      "100%|██████████| 79/79 [00:05<00:00, 14.84it/s]\n",
      "100%|██████████| 393/393 [00:42<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "A/acc: 0.5398089171974523\n",
      "B/acc: 0.47770700636942676\n",
      "retain/acc: 0.5796178343949044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from relearn.evaluate import run_eval\n",
    "\n",
    "res = run_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dict,\n",
    "    0,\n",
    "    use_wandb=False\n",
    ")\n",
    "for k, v in res.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
