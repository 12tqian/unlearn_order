{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# from research_tools.gpu import get_gpus_available\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# get rid of linker warnings that show up for some reason\n",
    "os.environ[\"OTHER_LDFLAGS\"] = \"-w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf27ae36eb145b1bbb0e2093427f762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from research_tools.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"LLM-LAT/zephyr7b-beta-rmu-lat-unlearn-wmdp-bio-cyber\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_id = \"google/gemma-7b\"\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, token=hf_access_token\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.datasets.utils import DATASETS_DICT, Datasets\n",
    "\n",
    "\n",
    "dataset_config = DATASETS_DICT[Datasets.WMDP_MCQ_CORPUS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ascent on corpus split, descent on wikitext\n",
    "# eval loss on regular split mcq val split, wikitext val split\n",
    "# also should eval on train split mcq val split, wikitext, mmlu cats\n",
    "# all val files should be mcq\n",
    "# all train should be corpus\n",
    "# unless u do rtt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from unlearn_order.dataset import load_dataset as load_dataset_unlearn\n",
    "\n",
    "retain_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "\n",
    "forget_train = load_dataset_unlearn(data_dir, dataset_config[\"unlearn_files\"])\n",
    "forget_val = load_dataset_unlearn(data_dir, dataset_config[\"val_files\"])\n",
    "\n",
    "# retain_train = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")[\"validation\"]\n",
    "\n",
    "retain_train = load_dataset_unlearn(data_dir, dataset_config[\"retain_files\"])\n",
    "retain_val = load_dataset_unlearn(data_dir, dataset_config[\"val_retain_files\"])\n",
    "\n",
    "val_files = dataset_config[\"val_files\"]\n",
    "n_val_files = 4\n",
    "forget_val_1 = load_dataset_unlearn(data_dir, val_files[:n_val_files])\n",
    "forget_val_2 = load_dataset_unlearn(data_dir, val_files[n_val_files:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.utils import (\n",
    "    create_prompt_letter_answer,\n",
    "    create_prompt,\n",
    "    create_prompt_question_answer,\n",
    "    create_prompt_answer_only,\n",
    ")\n",
    "\n",
    "completion_func = create_prompt_letter_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c921deccee2e471a92b346e71b52539c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c52d8ff34a43429e36ece3921faa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0182e4a352a2428f97db59657bba37f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2615c245d7ec44d79369d5ce484ef5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def map_corpus(data: Dict, tokenizer: AutoTokenizer, max_length: int):\n",
    "    text = data[\"text\"]\n",
    "    output = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": output[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": output[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": output[\"input_ids\"].clone().squeeze(),\n",
    "    }\n",
    "\n",
    "\n",
    "def process_train_dataset(\n",
    "    dataset: Dataset,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    max_length: int = 512,\n",
    "    min_length: int = 0,\n",
    "):\n",
    "    dataset = dataset.filter(lambda x: len(x[\"text\"]) > min_length)\n",
    "    dataset = dataset.map(\n",
    "        partial(map_corpus, tokenizer=tokenizer, max_length=max_length)\n",
    "    )\n",
    "\n",
    "    keep_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    dataset = dataset.remove_columns(\n",
    "        [col for col in dataset.column_names if col not in keep_cols]\n",
    "    )\n",
    "    dataset.set_format(\"torch\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "max_length = 512\n",
    "forget_train = process_train_dataset(forget_train, tokenizer, max_length=max_length)\n",
    "retain_train = process_train_dataset(retain_train, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def create_mcq(\n",
    "    record: Dict, tokenizer: AutoTokenizer, max_length: int, context: str = \"\"\n",
    "):\n",
    "    record[\"question\"] = context + record[\"question\"]\n",
    "    text = completion_func(record)\n",
    "    prompt = create_prompt(record)\n",
    "\n",
    "    completion = text[len(prompt) :]\n",
    "\n",
    "    record[\"prompt\"] = prompt\n",
    "    record[\"text\"] = text\n",
    "    record[\"completion\"] = completion\n",
    "\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
    "    completion_ids = tokenizer(\n",
    "        completion, return_tensors=\"pt\", add_special_tokens=False\n",
    "    ).input_ids[0]\n",
    "\n",
    "    input_ids = torch.cat([prompt_ids, completion_ids])\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    prompt_mask = torch.zeros_like(input_ids)\n",
    "    prompt_mask[: len(prompt_ids)] = 1\n",
    "\n",
    "    completion_mask = torch.zeros_like(input_ids)\n",
    "    completion_mask[len(prompt_ids) :] = 1\n",
    "\n",
    "    # pad to max length on left\n",
    "    seq_len = input_ids.size(0)\n",
    "    pad_len = max_length - seq_len\n",
    "    padding = (0, pad_len)\n",
    "\n",
    "    input_ids = F.pad(input_ids, padding, value=0)\n",
    "    labels = F.pad(labels, padding, value=-100)\n",
    "    attention_mask = F.pad(attention_mask, padding, value=0)\n",
    "    prompt_mask = F.pad(prompt_mask, padding, value=0)\n",
    "    completion_mask = F.pad(completion_mask, padding, value=0)\n",
    "\n",
    "    record[\"input_ids\"] = input_ids\n",
    "    record[\"labels\"] = labels\n",
    "    record[\"attention_mask\"] = attention_mask\n",
    "    record[\"completion_byte_len\"] = len(completion.encode(\"utf-8\"))\n",
    "    record[\"prompt_mask\"] = prompt_mask\n",
    "    record[\"completion_mask\"] = completion_mask\n",
    "    record[\"length\"] = seq_len\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "def expand_mcq_records(records: List[Dict], expand_choices: bool = True, **kwargs):\n",
    "    new_records = []\n",
    "    for rec in records:\n",
    "        n_choices = len(rec[\"choices\"])\n",
    "\n",
    "        if not expand_choices:\n",
    "            new_rec = deepcopy(rec)\n",
    "            new_rec = create_mcq(new_rec, **kwargs)\n",
    "            new_records.append(new_rec)\n",
    "            continue\n",
    "\n",
    "        for i in range(n_choices):\n",
    "            new_rec = deepcopy(rec)\n",
    "            actual_answer = rec[\"answer\"]\n",
    "            new_rec[\"answer\"] = i\n",
    "            new_rec = create_mcq(new_rec, **kwargs)\n",
    "            new_rec[\"answer\"] = actual_answer\n",
    "            new_rec[\"selected_answer\"] = i\n",
    "            new_records.append(new_rec)\n",
    "\n",
    "    return new_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_corpus_records(records: List[Dict]):\n",
    "    for rec in records:\n",
    "        rec[\"input_ids\"] = torch.tensor(rec[\"input_ids\"])\n",
    "        rec[\"labels\"] = torch.tensor(rec[\"labels\"])\n",
    "        rec[\"attention_mask\"] = torch.tensor(rec[\"attention_mask\"])\n",
    "        rec[\"length\"] = rec[\"input_ids\"].size(0)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "\n",
    "forget_train_records = forget_train.to_list()\n",
    "retain_train_records = retain_train.to_list()\n",
    "forget_val_records = forget_val.to_list()\n",
    "retain_val_records = retain_val.to_list()\n",
    "\n",
    "forget_train_records = expand_corpus_records(forget_train_records)\n",
    "retain_train_records = expand_corpus_records(retain_train_records)\n",
    "retain_val_records = expand_mcq_records(\n",
    "    retain_val_records, tokenizer=tokenizer, max_length=max_length\n",
    ")\n",
    "forget_val_records = expand_mcq_records(\n",
    "    forget_val_records, tokenizer=tokenizer, max_length=max_length\n",
    ")\n",
    "\n",
    "forget_val_1_records_train = expand_mcq_records(\n",
    "    forget_val_1.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    expand_choices=False,\n",
    ")\n",
    "forget_val_1_records = expand_mcq_records(\n",
    "    forget_val_1.to_list(), tokenizer=tokenizer, max_length=max_length\n",
    ")\n",
    "forget_val_2_records = expand_mcq_records(\n",
    "    forget_val_2.to_list(), tokenizer=tokenizer, max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fix_seq_len(batch: Dict, keys: List[str]):\n",
    "    max_seq_len = max(batch[\"length\"])\n",
    "    for key in keys:\n",
    "        batch[key] = batch[key][:, :max_seq_len]\n",
    "    return batch\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: AutoModelForCausalLM,\n",
    "    records: List[Dict],\n",
    "    batch_size: int = 8,\n",
    "    n_choices: int = 4,\n",
    "    normalize_loss: bool = False,\n",
    "):\n",
    "    # round up to nearest multiple of n_choices\n",
    "    batch_size = (batch_size + n_choices - 1) // n_choices * n_choices\n",
    "    original_fields = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "    aux_fields = [\"answer\", \"completion_byte_len\", \"completion_mask\", \"length\"]\n",
    "    fields = original_fields + aux_fields\n",
    "    dataset = [{k: v for k, v in rec.items() if k in fields} for rec in records]\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = fix_seq_len(batch, original_fields + [\"completion_mask\"])\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            completion_mask = batch[\"completion_mask\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device).bool()\n",
    "            completion_byte_len = batch[\"completion_byte_len\"].to(device)\n",
    "\n",
    "            answers = batch[\"answer\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            labels[~completion_mask.bool()] = -100\n",
    "            labels[~attention_mask.bool()] = -100\n",
    "\n",
    "            shifted_logits = logits[:, :-1, :].contiguous()\n",
    "            shifted_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            loss = F.cross_entropy(\n",
    "                shifted_logits.view(-1, shifted_logits.size(-1)),\n",
    "                shifted_labels.view(-1),\n",
    "                reduction=\"none\",\n",
    "            )\n",
    "            loss = loss.view(shifted_labels.size(0), shifted_labels.size(1))\n",
    "            loss_by_sample = loss.sum(dim=1)\n",
    "\n",
    "            if normalize_loss:\n",
    "                loss_by_sample = loss_by_sample / completion_byte_len\n",
    "            loss_by_sample = loss_by_sample.view(-1, n_choices)\n",
    "\n",
    "            answers = answers[::n_choices]\n",
    "\n",
    "            pred = loss_by_sample.argmin(dim=1)\n",
    "\n",
    "            correct += (pred == answers).sum().item()\n",
    "            total += len(answers)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from unlearn_order.utils import log_1_minus_p_loss, my_loss\n",
    "from unlearn_order.eval import eval_dataset as my_eval_dataset\n",
    "\n",
    "\n",
    "def train_step_gd(\n",
    "    model: AutoModelForCausalLM,\n",
    "    forget_batch: Dict,\n",
    "    retain_batch: Dict,\n",
    "    forget_alpha: float = 0.1,\n",
    "    use_log_1_minus_p: bool = True,\n",
    "):\n",
    "    forget_batch = fix_seq_len(forget_batch, [\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    retain_batch = fix_seq_len(retain_batch, [\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    forget_input_ids = forget_batch[\"input_ids\"].to(device)\n",
    "    forget_attention_mask = forget_batch[\"attention_mask\"].to(device)\n",
    "    forget_labels = forget_batch[\"labels\"].to(device)\n",
    "\n",
    "    retain_input_ids = retain_batch[\"input_ids\"].to(device)\n",
    "    retain_attention_mask = retain_batch[\"attention_mask\"].to(device)\n",
    "    retain_labels = retain_batch[\"labels\"].to(device)\n",
    "\n",
    "    forget_loss = my_loss(\n",
    "        model,\n",
    "        is_away=use_log_1_minus_p,\n",
    "        input_ids=forget_input_ids,\n",
    "        attention_mask=forget_attention_mask,\n",
    "        labels=forget_labels,\n",
    "    ) * (1 if use_log_1_minus_p else -1)\n",
    "\n",
    "    forget_loss = forget_loss * forget_alpha\n",
    "    forget_loss.backward()\n",
    "\n",
    "    retain_loss = my_loss(\n",
    "        model,\n",
    "        is_away=False,\n",
    "        input_ids=retain_input_ids,\n",
    "        attention_mask=retain_attention_mask,\n",
    "        labels=retain_labels,\n",
    "    )\n",
    "\n",
    "    retain_loss.backward()\n",
    "\n",
    "    loss = forget_loss.detach() + retain_loss.detach()\n",
    "    loss = loss.detach().item()\n",
    "    forget_loss = forget_loss.detach().item()\n",
    "    retain_loss = retain_loss.detach().item()\n",
    "\n",
    "    loss_dict = {\n",
    "        \"loss\": loss,\n",
    "        \"forget_loss\": forget_loss,\n",
    "        \"retain_loss\": retain_loss,\n",
    "    }\n",
    "\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_gd(\n",
    "    model: AutoModelForCausalLM,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    forget_train_records: List[Dict],\n",
    "    retain_train_records: List[Dict],\n",
    "    batch_size: int = 4,\n",
    "    forget_alpha: float = 0.1,\n",
    "    lr: float = 3e-5,\n",
    "    log_steps: int = 50,\n",
    "    grad_accum_steps: int = 1,\n",
    "    use_log_1_minus_p: bool = True,\n",
    "):\n",
    "    model.train()\n",
    "    forget_dataloader = DataLoader(\n",
    "        forget_train_records, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    retain_dataloader = DataLoader(\n",
    "        retain_train_records, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    loss_traj = []\n",
    "    for step, (forget_batch, retain_batch) in tqdm(\n",
    "        enumerate(zip(forget_dataloader, retain_dataloader))\n",
    "    ):\n",
    "        loss_dict = train_step_gd(\n",
    "            model, forget_batch, retain_batch, forget_alpha, use_log_1_minus_p\n",
    "        )\n",
    "\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_traj.append(loss_dict)\n",
    "        if (step + 1) % log_steps == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Step {step}, Loss {loss_dict['loss']}, Forget Loss {loss_dict['forget_loss']}, Retain Loss {loss_dict['retain_loss']}\"\n",
    "            )\n",
    "\n",
    "    return loss_traj\n",
    "\n",
    "\n",
    "def train_gd(\n",
    "    model: AutoModelForCausalLM,\n",
    "    n_epochs: int,\n",
    "    forget_train_records: List[Dict],\n",
    "    retain_train_records: List[Dict],\n",
    "    forget_val_records: List[Dict],\n",
    "    retain_val_records: List[Dict],\n",
    "    batch_size: int = 4,\n",
    "    forget_alpha: float = 0.1,\n",
    "    lr: float = 3e-5,\n",
    "    log_steps: int = 50,\n",
    "    eval_at_start: bool = True,\n",
    "    grad_accum_steps: int = 1,\n",
    "    use_log_1_minus_p: bool = False,\n",
    "):\n",
    "    if eval_at_start:\n",
    "        retain_acc = evaluate(\n",
    "            model, retain_val_records, batch_size=8, normalize_loss=False\n",
    "        )\n",
    "        forget_acc = evaluate(\n",
    "            model, forget_val_records, batch_size=8, normalize_loss=False\n",
    "        )\n",
    "\n",
    "        print(f\"Initial Retain Accuracy: {retain_acc}\")\n",
    "        print(f\"Initial Forget Accuracy: {forget_acc}\")\n",
    "\n",
    "    layers = list(range(2, 7 + 1))\n",
    "    params = []\n",
    "    for layer in layers:\n",
    "        params.extend(model.model.layers[layer].mlp.parameters())\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False \n",
    "    \n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_epoch_gd(\n",
    "            model,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            forget_train_records,\n",
    "            retain_train_records,\n",
    "            batch_size,\n",
    "            forget_alpha,\n",
    "            lr=lr,\n",
    "            log_steps=log_steps,\n",
    "            grad_accum_steps=grad_accum_steps,\n",
    "            use_log_1_minus_p=use_log_1_minus_p,\n",
    "        )\n",
    "        retain_acc = evaluate(\n",
    "            model, retain_val_records, batch_size=8, normalize_loss=False\n",
    "        )\n",
    "        forget_acc = evaluate(\n",
    "            model, forget_val_records, batch_size=8, normalize_loss=False\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Retain Accuracy: {retain_acc}\")\n",
    "        print(f\"Epoch: {epoch}, Forget Accuracy: {forget_acc}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_train_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_train_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_val_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_val_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_at_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_log_1_minus_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 79\u001b[0m, in \u001b[0;36mtrain_gd\u001b[0;34m(model, n_epochs, forget_train_records, retain_train_records, forget_val_records, retain_val_records, batch_size, forget_alpha, lr, log_steps, eval_at_start, grad_accum_steps, use_log_1_minus_p)\u001b[0m\n\u001b[1;32m     76\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params, lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mtrain_epoch_gd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforget_train_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_train_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforget_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_log_1_minus_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_log_1_minus_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     retain_acc \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     93\u001b[0m         model, retain_val_records, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, normalize_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     95\u001b[0m     forget_acc \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     96\u001b[0m         model, forget_val_records, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, normalize_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mtrain_epoch_gd\u001b[0;34m(model, optimizer, epoch, forget_train_records, retain_train_records, batch_size, forget_alpha, lr, log_steps, grad_accum_steps, use_log_1_minus_p)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss_traj \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (forget_batch, retain_batch) \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(forget_dataloader, retain_dataloader))\n\u001b[1;32m     24\u001b[0m ):\n\u001b[0;32m---> 25\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_gd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforget_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_log_1_minus_p\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m grad_accum_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m, in \u001b[0;36mtrain_step_gd\u001b[0;34m(model, forget_batch, retain_batch, forget_alpha, use_log_1_minus_p)\u001b[0m\n\u001b[1;32m     25\u001b[0m forget_loss \u001b[38;5;241m=\u001b[39m my_loss(\n\u001b[1;32m     26\u001b[0m     model,\n\u001b[1;32m     27\u001b[0m     is_away\u001b[38;5;241m=\u001b[39muse_log_1_minus_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     labels\u001b[38;5;241m=\u001b[39mforget_labels,\n\u001b[1;32m     31\u001b[0m ) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_log_1_minus_p \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m forget_loss \u001b[38;5;241m=\u001b[39m forget_loss \u001b[38;5;241m*\u001b[39m forget_alpha\n\u001b[0;32m---> 34\u001b[0m \u001b[43mforget_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m retain_loss \u001b[38;5;241m=\u001b[39m my_loss(\n\u001b[1;32m     37\u001b[0m     model,\n\u001b[1;32m     38\u001b[0m     is_away\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     labels\u001b[38;5;241m=\u001b[39mretain_labels,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m retain_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "model = train_gd(\n",
    "    model,\n",
    "    4,\n",
    "    forget_train_records,\n",
    "    retain_train_records,\n",
    "    forget_val_records,\n",
    "    retain_val_records,\n",
    "    4,\n",
    "    forget_alpha=1,\n",
    "    eval_at_start=False,\n",
    "    log_steps=50,\n",
    "    lr=1e-5,\n",
    "    grad_accum_steps=8,\n",
    "    use_log_1_minus_p=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from unlearn_order.utils import log_1_minus_p_loss, my_loss\n",
    "\n",
    "\n",
    "def train_step_ft(\n",
    "    model: AutoModelForCausalLM,\n",
    "    batch: Dict,\n",
    "):\n",
    "    batch = fix_seq_len(\n",
    "        batch, [\"input_ids\", \"attention_mask\", \"labels\", \"completion_mask\"]\n",
    "    )\n",
    "\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    completion_mask = batch[\"completion_mask\"].to(device)\n",
    "\n",
    "    # only train on completions for multiple choice\n",
    "    labels[~completion_mask.bool()] = -100\n",
    "\n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = output.loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    loss = loss.detach().item()\n",
    "\n",
    "    loss_dict = {\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318e11a7bf544c32a9f1b7be0029e96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save model to ../models/ directory\n",
    "model_dir = Path(\"../models\")\n",
    "# load model from checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.bfloat16).to(\n",
    "    device\n",
    ")\n",
    "# model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# save checkpoint\n",
    "# model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ft(\n",
    "    model: AutoModelForCausalLM,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    mcq_records: List[Dict],\n",
    "    batch_size: int = 4,\n",
    "    lr: float = 3e-5,\n",
    "    log_steps: int = 50,\n",
    "    grad_accum_steps: int = 1,\n",
    "):\n",
    "    model.train()\n",
    "    dataloader = DataLoader(mcq_records, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_traj = []\n",
    "    for step, batch in tqdm(enumerate(dataloader)):\n",
    "        loss_dict = train_step_ft(model, batch)\n",
    "\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_traj.append(loss_dict)\n",
    "        if (step + 1) % log_steps == 0:\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss {loss_dict['loss']}\")\n",
    "\n",
    "    return loss_traj\n",
    "\n",
    "\n",
    "def train_ft(\n",
    "    model: AutoModelForCausalLM,\n",
    "    n_epochs: int,\n",
    "    mcq_records: List[Dict],\n",
    "    forget_val_1_records: List[Dict],\n",
    "    forget_val_2_records: List[Dict],\n",
    "    batch_size: int = 4,\n",
    "    lr: float = 3e-5,\n",
    "    log_steps: int = 50,\n",
    "    eval_at_start: bool = True,\n",
    "    grad_accum_steps: int = 1,\n",
    "):\n",
    "    if eval_at_start:\n",
    "        forget_acc_1 = evaluate(\n",
    "            model, forget_val_1_records, batch_size=8, normalize_loss=False\n",
    "        )\n",
    "        forget_acc_2 = evaluate(\n",
    "            model, forget_val_2_records, batch_size=8, normalize_loss=False\n",
    "        )\n",
    "\n",
    "        print(f\"Initial Forget accuracy 1: {forget_acc_1}\")\n",
    "        print(f\"Initial Forget accuracy 2: {forget_acc_2}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_epoch_ft(\n",
    "            model,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            mcq_records,\n",
    "            batch_size,\n",
    "            lr=lr,\n",
    "            log_steps=log_steps,\n",
    "            grad_accum_steps=grad_accum_steps,\n",
    "        )\n",
    "        forget_acc_1 = evaluate(\n",
    "            model, forget_val_1_records, batch_size=8, normalize_loss=False\n",
    "        )\n",
    "        forget_acc_2 = evaluate(\n",
    "            model, forget_val_2_records, batch_size=8, normalize_loss=False\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch}, Forget accuracy 1: {forget_acc_1}\")\n",
    "        print(f\"Epoch {epoch}, Forget accuracy 2: {forget_acc_2}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:14,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 49, Loss 0.33920395374298096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:29,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 99, Loss 0.10235144197940826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:43,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 149, Loss 0.06902649998664856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:45,  3.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [00:20<00:00, 15.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:05<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Forget accuracy 1: 0.5828025477707006\n",
      "Epoch 0, Forget accuracy 2: 0.39490445859872614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:14,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 49, Loss 0.14688843488693237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:29,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 99, Loss 0.056751690804958344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:43,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 149, Loss 0.09005525708198547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:45,  3.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [00:20<00:00, 15.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:05<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Forget accuracy 1: 0.6305732484076433\n",
      "Epoch 1, Forget accuracy 2: 0.45222929936305734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:14,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 49, Loss 0.04935349524021149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:29,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 99, Loss 0.09599190950393677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:43,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 149, Loss 0.102555051445961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:45,  3.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [00:20<00:00, 15.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:05<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Forget accuracy 1: 0.6719745222929936\n",
      "Epoch 2, Forget accuracy 2: 0.4585987261146497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:14,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 49, Loss 0.024202320724725723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:29,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 99, Loss 0.12140986323356628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:43,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 149, Loss 0.12829658389091492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:45,  3.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [00:20<00:00, 15.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:05<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Forget accuracy 1: 0.7261146496815286\n",
      "Epoch 3, Forget accuracy 2: 0.4840764331210191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:14,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 49, Loss 0.07507164031267166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:29,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 99, Loss 0.05780886113643646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:43,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 149, Loss 0.07962491363286972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:46,  3.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [00:20<00:00, 15.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:05<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Forget accuracy 1: 0.7945859872611465\n",
      "Epoch 4, Forget accuracy 2: 0.49044585987261147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:14,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 49, Loss 0.032094892114400864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:29,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 99, Loss 0.002891097217798233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:43,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 149, Loss 0.052811723202466965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:45,  3.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 314/314 [00:20<00:00, 15.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:05<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Forget accuracy 1: 0.8885350318471338\n",
      "Epoch 5, Forget accuracy 2: 0.49044585987261147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:14,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 49, Loss 0.03581025078892708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:21,  3.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:21,  3.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_ft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_val_1_records_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_val_1_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_val_2_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_at_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 55\u001b[0m, in \u001b[0;36mtrain_ft\u001b[0;34m(model, n_epochs, mcq_records, forget_val_1_records, forget_val_2_records, batch_size, lr, log_steps, eval_at_start, grad_accum_steps)\u001b[0m\n\u001b[1;32m     52\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mtrain_epoch_ft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmcq_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     forget_acc_1 \u001b[38;5;241m=\u001b[39m evaluate(model, forget_val_1_records, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, normalize_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m     forget_acc_2 \u001b[38;5;241m=\u001b[39m evaluate(model, forget_val_2_records, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, normalize_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m, in \u001b[0;36mtrain_epoch_ft\u001b[0;34m(model, optimizer, epoch, mcq_records, batch_size, lr, log_steps, grad_accum_steps)\u001b[0m\n\u001b[1;32m     16\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m train_step_ft(\n\u001b[1;32m     17\u001b[0m     model, batch\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m grad_accum_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m loss_traj\u001b[38;5;241m.\u001b[39mappend(loss_dict)\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/optim/adam.py:606\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    604\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    605\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n\u001b[0;32m--> 606\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_addcdiv_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_avg_sq_sqrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    608\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ft(\n",
    "    model,\n",
    "    10,\n",
    "    forget_val_1_records_train,\n",
    "    forget_val_1_records,\n",
    "    forget_val_2_records,\n",
    "    batch_size=4,\n",
    "    lr=1e-6,\n",
    "    log_steps=50,\n",
    "    eval_at_start=False,\n",
    "    grad_accum_steps=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
