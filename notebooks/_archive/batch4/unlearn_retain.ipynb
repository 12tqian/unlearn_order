{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# from research_tools.gpu import get_gpus_available\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c2586c38194d08975c3e6404069a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from research_tools.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"LLM-LAT/zephyr7b-beta-rmu-lat-unlearn-wmdp-bio-cyber\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, token=hf_access_token\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b830a203c341f5886b671f78b8ae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41238d56c9744269b12b0547a8d3714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99c7299a17a43fbab6cb7b90450c6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dabde59808417cba4cd797a85339f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211fa68daf604aa09235a25d5b0bb7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de87cc075114fc89e400e92025847e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from relearn.datasets.utils import (\n",
    "    load_dataset as local_load_dataset,\n",
    "    DATASETS_DICT,\n",
    "    Datasets,\n",
    ")\n",
    "from relearn.datasets.corpus import process as process_corpus\n",
    "from relearn.datasets.mcq import process as process_mcq\n",
    "\n",
    "dataset_config = DATASETS_DICT[Datasets.WMDP]\n",
    "\n",
    "# retain_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "\n",
    "retain_train = local_load_dataset(data_dir, dataset_config[\"retain_files\"])\n",
    "retain_val = local_load_dataset(data_dir, dataset_config[\"val_retain_files\"])\n",
    "\n",
    "unlearn_files = dataset_config[\"unlearn_files\"]\n",
    "val_unlearn_files = dataset_config[\"val_unlearn_files\"]\n",
    "\n",
    "n_val_files = 4\n",
    "max_length = 512\n",
    "\n",
    "forget_train_1 = local_load_dataset(data_dir, unlearn_files[:n_val_files])\n",
    "forget_train_2 = local_load_dataset(data_dir, unlearn_files[n_val_files:])\n",
    "\n",
    "forget_val_1 = local_load_dataset(data_dir, val_unlearn_files[:n_val_files])\n",
    "forget_val_2 = local_load_dataset(data_dir, val_unlearn_files[n_val_files:])\n",
    "\n",
    "forget_train_1_records = process_corpus(forget_train_1, tokenizer, max_length)\n",
    "forget_train_2_records = process_corpus(forget_train_2, tokenizer, max_length)\n",
    "retain_train_records = process_corpus(retain_train, tokenizer, max_length)\n",
    "forget_train_records = forget_train_1_records + forget_train_2_records\n",
    "\n",
    "forget_train_mcq_1_records = process_mcq(\n",
    "    forget_val_1, tokenizer, max_length, expand_choices=False\n",
    ")\n",
    "forget_train_mcq_2_records = process_mcq(\n",
    "    forget_val_2, tokenizer, max_length, expand_choices=False\n",
    ")\n",
    "forget_train_mcq_records = forget_train_mcq_1_records + forget_train_mcq_2_records\n",
    "\n",
    "forget_val_1_records = process_mcq(forget_val_1, tokenizer, max_length)\n",
    "forget_val_2_records = process_mcq(forget_val_2, tokenizer, max_length)\n",
    "retain_val_records = process_mcq(retain_val, tokenizer, max_length)\n",
    "forget_val_records = forget_val_1_records + forget_val_2_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relearn.unlearn.gd import train_gd\n",
    "\n",
    "model = train_gd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/314 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|██████████| 314/314 [00:18<00:00, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start forget 1 Accuracy: 0.39490445859872614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 16.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start forget 2 Accuracy: 0.33121019108280253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:08,  3.51it/s]"
     ]
    }
   ],
   "source": [
    "from relearn.attacks.rtt import train_ft\n",
    "\n",
    "# relearm only A\n",
    "model = train_ft(\n",
    "    model,\n",
    "    10,\n",
    "    forget_train_mcq_1_records,\n",
    "    {\n",
    "        \"forget 1\": forget_val_1_records,\n",
    "        \"forget 2\": forget_val_2_records,\n",
    "    },\n",
    "    batch_size=4,\n",
    "    lr=1e-6,\n",
    "    eval_at_start=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
