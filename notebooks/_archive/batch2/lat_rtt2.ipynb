{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_tools.gpu import get_gpus_available\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3464c5369cdf4206a829d976a91d513d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, token=hf_access_token\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6012658227848101\n"
     ]
    }
   ],
   "source": [
    "from unlearn_order.dataset import load_dataset\n",
    "from unlearn_order.eval import eval_dataset\n",
    "\n",
    "data_dir = Path(\"../data/dates-years-trimmed\")\n",
    "n_train = 3\n",
    "n_val = 2\n",
    "batch_size = 4\n",
    "train_files = [f\"split_{i}.jsonl\" for i in range(n_train)]\n",
    "val_files = [f\"split_{i}.jsonl\" for i in range(n_train, n_train + n_val)]\n",
    "train_dataset = load_dataset(data_dir, train_files)\n",
    "val_dataset = load_dataset(data_dir, val_files)\n",
    "\n",
    "\n",
    "acc = eval_dataset(model, tokenizer, train_dataset, batch_size=batch_size)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first we want to finetune on the dataset\n",
    "\n",
    "# from unlearn_order.finetune import finetune_model\n",
    "\n",
    "\n",
    "# # with profile(activities=[ProfilerActivity.CPU],\n",
    "# #         profile_memory=True, record_shapes=True) as prof:\n",
    "# #     model(inputs)\n",
    "\n",
    "# # print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
    "# model = finetune_model(model, tokenizer, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearn_train_files = [f\"corpus_split_{i}.jsonl\" for i in range(n_train)]\n",
    "unlearn_corpus = load_dataset(data_dir, unlearn_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = unlearn_corpus[\"text\"]\n",
    "data_list = [\n",
    "    data_list[i : i + batch_size] for i in range(0, len(data_list), batch_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "unlearning_task = \"cyber\"\n",
    "\n",
    "forget_corpus = (\n",
    "    \"cyber-forget-corpus\" if unlearning_task == \"cyber\" else \"bio-forget-corpus\"\n",
    ")\n",
    "retain_corpus = \"wikitext\"\n",
    "\n",
    "\n",
    "def get_unlearning_rmu_dataset(name, min_len=0, batch_size=4):\n",
    "    data = []\n",
    "    if name == \"wikitext\":\n",
    "        raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    elif name == \"cyber-forget-corpus\":\n",
    "        raw_data = load_dataset(\"cais/wmdp-corpora\", name=name, split=\"train\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    else:  # wmdp bio must be pre-downloaded\n",
    "        for line in open(f\"data/{name}.jsonl\", \"r\"):\n",
    "            raw_text = json.loads(line)[\"text\"]\n",
    "            if len(raw_text) > min_len:\n",
    "                data.append(str(raw_text))\n",
    "    data = [data[i : i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    return data\n",
    "\n",
    "\n",
    "forget_data_list = data_list\n",
    "retain_data_list = get_unlearning_rmu_dataset(retain_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from unlearn_order.algos.lat.hooks import add_hooks\n",
    "from unlearn_order.algos.lat.model import get_adversary_hook, projected_gradient_descent\n",
    "from unlearn_order.algos.lat.utils import (\n",
    "    get_layer_module,\n",
    "    set_model_grads,\n",
    "    set_params_grads,\n",
    ")\n",
    "from typing import List, Dict\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "def get_layers_params(\n",
    "    model: LlamaForCausalLM, layers: List[int], target_modules: List[str]\n",
    "):\n",
    "    params = []\n",
    "    for layer in layers:\n",
    "        for name, module in model.model.layers[layer].named_modules():\n",
    "            if any([target in name for target in target_modules]):\n",
    "                for param in module.parameters():\n",
    "                    params.append(param)\n",
    "    return params\n",
    "\n",
    "\n",
    "def forward_with_cache(\n",
    "    model: LlamaForCausalLM,\n",
    "    inputs: Dict[str, torch.Tensor],\n",
    "    module: torch.nn.Module,\n",
    "    no_grad: bool = True,\n",
    "):\n",
    "    # define a tensor with the size of our cached activations\n",
    "    cache = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            cache.append(output[0])\n",
    "        else:\n",
    "            cache.append(output)\n",
    "        return None\n",
    "\n",
    "    hook_handle = module.register_forward_hook(hook)\n",
    "\n",
    "    if no_grad:\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    else:\n",
    "        _ = model(**inputs)\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    return cache[0]\n",
    "\n",
    "\n",
    "# you're optimizing layer_ids\n",
    "# there is a layer where you're examining the activations at and trying to make garbage\n",
    "# then you only train on the laeyrs before that, this is just shoving garbage in\n",
    "# not real unlearning?\n",
    "# let us also latently perturb at the same layer\n",
    "\n",
    "\n",
    "# remember params\n",
    "def run_rmu(\n",
    "    updated_model: LlamaForCausalLM,\n",
    "    frozen_model: LlamaForCausalLM,\n",
    "    tokenizer: LlamaTokenizer,\n",
    "    forget_data_list,\n",
    "    retain_data_list,\n",
    "    target_modules: List[str] = [\"down_proj\"],\n",
    "    alpha: float = 1200.0,\n",
    "    layer: int = 8,  # layers to do RMU in\n",
    "    lr_def: float = 5.0e-5,\n",
    "    max_num_batches: int = 200,\n",
    "    steering_coef: float = 6.5,\n",
    "    use_lat: bool = True,\n",
    "    epsilon: float = 2,\n",
    "    lr_adv: float = 5e-2,\n",
    "    n_pgd_steps: int = 16,\n",
    "    loss_coefs: Dict[str, float] = {\"towards\": 1, \"away\": 1},\n",
    "    num_epochs: int = 1,\n",
    "    n_def_per_adv_steps: int = 1,\n",
    "):\n",
    "    adv_layer = layer - 1\n",
    "    def_layers = [layer, layer - 1, layer - 2]\n",
    "\n",
    "    updated_model.train()\n",
    "\n",
    "    params = get_layers_params(updated_model, def_layers, target_modules)\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr_def)\n",
    "\n",
    "    updated_module = get_layer_module(updated_model, layer)\n",
    "    frozen_module = get_layer_module(frozen_model, layer)\n",
    "\n",
    "    control_vectors_list = []\n",
    "    for i in range(len(forget_data_list)):\n",
    "        random_vector = torch.rand(\n",
    "            1,\n",
    "            1,\n",
    "            updated_model.config.hidden_size,\n",
    "            dtype=updated_model.dtype,\n",
    "            device=updated_model.device,\n",
    "        )\n",
    "        control_vec = random_vector / torch.norm(random_vector) * steering_coef\n",
    "        control_vectors_list.append(control_vec)\n",
    "\n",
    "    num_batches = max_num_batches\n",
    "    truncation_side = tokenizer.truncation_side\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.truncation_side = \"right\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx in tqdm(range(num_batches)):\n",
    "            set_params_grads(params, True)\n",
    "\n",
    "            control_vec = control_vectors_list[idx]\n",
    "            unlearn_batch = forget_data_list[idx]\n",
    "            retain_batch = retain_data_list[idx]\n",
    "\n",
    "            max_length = 512 if idx == 0 else 768\n",
    "\n",
    "            unlearn_inputs = tokenizer(\n",
    "                unlearn_batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            retain_inputs = tokenizer(\n",
    "                retain_batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            if use_lat:\n",
    "                adv_labels_mask = torch.zeros_like(\n",
    "                    unlearn_inputs[\"input_ids\"], dtype=bool\n",
    "                )\n",
    "                def_labels_mask = torch.zeros_like(\n",
    "                    retain_inputs[\"input_ids\"], dtype=bool\n",
    "                )\n",
    "\n",
    "                for b, example in enumerate(retain_batch):\n",
    "                    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "                    def_labels_mask[b, :len_example] = True\n",
    "                for b, example in enumerate(unlearn_batch):\n",
    "                    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "                    adv_labels_mask[b, :len_example] = True\n",
    "\n",
    "                prompt_mask = adv_labels_mask\n",
    "                pgd_batch = {\n",
    "                    \"def_tokens\": retain_inputs[\"input_ids\"].to(updated_model.device),\n",
    "                    \"adv_tokens\": unlearn_inputs[\"input_ids\"].to(updated_model.device),\n",
    "                    \"adv_labels_mask\": adv_labels_mask.to(updated_model.device),\n",
    "                    \"def_labels_mask\": def_labels_mask.to(updated_model.device),\n",
    "                    \"prompt_mask\": prompt_mask.to(updated_model.device),\n",
    "                }\n",
    "                adversary = projected_gradient_descent(\n",
    "                    updated_model,\n",
    "                    pgd_batch,\n",
    "                    layer=adv_layer,\n",
    "                    epsilon=epsilon,\n",
    "                    n_pgd_steps=n_pgd_steps,\n",
    "                    lr_adv=lr_adv,\n",
    "                    loss_coefs=loss_coefs,\n",
    "                )\n",
    "\n",
    "                set_model_grads(adversary, False)\n",
    "                set_params_grads(params, True)\n",
    "\n",
    "                for step in range(n_def_per_adv_steps):\n",
    "\n",
    "                    # Unlearning loss\n",
    "                    with add_hooks(\n",
    "                        module_forward_hooks=[\n",
    "                            (\n",
    "                                get_layer_module(updated_model, layer),\n",
    "                                get_adversary_hook(adversary),\n",
    "                            )\n",
    "                        ],\n",
    "                        module_forward_pre_hooks=[],\n",
    "                    ):\n",
    "                        updated_forget_activations = forward_with_cache(\n",
    "                            updated_model,\n",
    "                            unlearn_inputs,\n",
    "                            module=updated_module,\n",
    "                            no_grad=False,\n",
    "                        ).to(updated_model.device)\n",
    "\n",
    "                    unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_forget_activations, control_vec\n",
    "                    )\n",
    "\n",
    "                    # Retain loss\n",
    "                    updated_retain_activations = forward_with_cache(\n",
    "                        updated_model,\n",
    "                        retain_inputs,\n",
    "                        module=updated_module,\n",
    "                        no_grad=False,\n",
    "                    ).to(updated_model.device)\n",
    "                    frozen_retain_activations = forward_with_cache(\n",
    "                        frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                    ).to(updated_model.device)\n",
    "\n",
    "                    retain_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_retain_activations, frozen_retain_activations\n",
    "                    )\n",
    "                    retain_loss *= alpha\n",
    "\n",
    "                    # Update model\n",
    "                    loss = unlearn_loss + retain_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    # Unlearning loss\n",
    "                    updated_forget_activations = forward_with_cache(\n",
    "                        updated_model,\n",
    "                        unlearn_inputs,\n",
    "                        module=updated_module,\n",
    "                        no_grad=False,\n",
    "                    ).to(updated_model.device)\n",
    "                    unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_forget_activations, control_vec\n",
    "                    )\n",
    "\n",
    "                    # Retain loss\n",
    "                    updated_retain_activations = forward_with_cache(\n",
    "                        updated_model,\n",
    "                        retain_inputs,\n",
    "                        module=updated_module,\n",
    "                        no_grad=False,\n",
    "                    ).to(updated_model.device)\n",
    "                    frozen_retain_activations = forward_with_cache(\n",
    "                        frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                    ).to(updated_model.device)\n",
    "\n",
    "                    retain_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_retain_activations, frozen_retain_activations\n",
    "                    )\n",
    "                    retain_loss *= alpha\n",
    "\n",
    "                    # Update model\n",
    "                    loss = unlearn_loss + retain_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Batch: {idx}, Unlearn Loss: {unlearn_loss.item()}, Retain Loss: {retain_loss.item()}, Total Loss: {loss.item()}\"\n",
    "                )\n",
    "                acc = eval_dataset(\n",
    "                    updated_model, tokenizer, train_dataset, batch_size=batch_size\n",
    "                )\n",
    "                print(f\"Validation Accuracy: {acc}\")\n",
    "\n",
    "    tokenizer.truncation_side = truncation_side\n",
    "\n",
    "    return updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c45390826f4ca2aabd05dc61089d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "/tmp/ipykernel_3507659/2557494747.py:192: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 512, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "/tmp/ipykernel_3507659/2557494747.py:225: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 512, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Unlearn Loss: 0.0849609375, Retain Loss: 0.173828125, Total Loss: 0.2578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:23<1:16:52, 23.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6054852320675106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3507659/2557494747.py:192: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 768, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "/tmp/ipykernel_3507659/2557494747.py:225: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 768, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "  5%|▌         | 10/200 [02:22<42:33, 13.44s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 10, Unlearn Loss: 0.06103515625, Retain Loss: 0.004302978515625, Total Loss: 0.0654296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [02:50<55:50, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5970464135021097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [04:50<40:36, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Unlearn Loss: 0.05859375, Retain Loss: 0.0035552978515625, Total Loss: 0.062255859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [05:26<1:00:50, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6033755274261603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 10%|█         | 21/200 [05:27<46:28, 15.58s/it]  \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 10.75 MiB is free. Including non-PyTorch memory, this process has 35.60 GiB memory in use. Process 3510749 has 43.47 GiB memory in use. Of the allocated memory 34.59 GiB is allocated by PyTorch, and 378.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m updated_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m      2\u001b[0m frozen_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m     model_id, revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m updated_model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_rmu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdated_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrozen_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# function to make another version of the model for reference embeddings\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# tokenizer\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforget_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# forget data\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# retain data\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_lat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_def_per_adv_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_pgd_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_num_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 160\u001b[0m, in \u001b[0;36mrun_rmu\u001b[0;34m(updated_model, frozen_model, tokenizer, forget_data_list, retain_data_list, target_modules, alpha, layer, lr_def, max_num_batches, steering_coef, use_lat, epsilon, lr_adv, n_pgd_steps, loss_coefs, num_epochs, n_def_per_adv_steps)\u001b[0m\n\u001b[1;32m    152\u001b[0m prompt_mask \u001b[38;5;241m=\u001b[39m adv_labels_mask\n\u001b[1;32m    153\u001b[0m pgd_batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: retain_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(updated_model\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madv_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: unlearn_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(updated_model\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_mask\u001b[38;5;241m.\u001b[39mto(updated_model\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    159\u001b[0m }\n\u001b[0;32m--> 160\u001b[0m adversary \u001b[38;5;241m=\u001b[39m \u001b[43mprojected_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdated_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpgd_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madv_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_pgd_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_pgd_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_adv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_adv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_coefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_coefs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m set_model_grads(adversary, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m set_params_grads(params, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/src/unlearn_order/algos/lat/model.py:162\u001b[0m, in \u001b[0;36mprojected_gradient_descent\u001b[0;34m(model, batch, layer, attack_completions, epsilon, n_pgd_steps, lr_adv, loss_coefs)\u001b[0m\n\u001b[1;32m    154\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m add_hooks(\n\u001b[1;32m    157\u001b[0m     module_forward_hooks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    158\u001b[0m         (get_layer_module(model, layer), get_adversary_hook(adversary))\n\u001b[1;32m    159\u001b[0m     ],\n\u001b[1;32m    160\u001b[0m     module_forward_pre_hooks\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    161\u001b[0m ):\n\u001b[0;32m--> 162\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mget_towards_away_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# TODO(tcqian): investigate gradient clipping and nans\u001b[39;00m\n\u001b[1;32m    165\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(combine_losses(losses, loss_coefs)\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/src/unlearn_order/algos/lat/model.py:114\u001b[0m, in \u001b[0;36mget_towards_away_loss\u001b[0;34m(model, batch, is_adv)\u001b[0m\n\u001b[1;32m    108\u001b[0m     towards_tokens, away_tokens \u001b[38;5;241m=\u001b[39m away_tokens, towards_tokens\n\u001b[1;32m    109\u001b[0m     towards_labels_mask, away_labels_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    110\u001b[0m         away_labels_mask,\n\u001b[1;32m    111\u001b[0m         towards_labels_mask,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m towards_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtowards_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtowards_labels_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m away_loss \u001b[38;5;241m=\u001b[39m get_token_loss(model, away_tokens, away_labels_mask, is_away\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m losses \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtowards\u001b[39m\u001b[38;5;124m\"\u001b[39m: towards_loss,\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maway\u001b[39m\u001b[38;5;124m\"\u001b[39m: away_loss,\n\u001b[1;32m    119\u001b[0m }\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/src/unlearn_order/algos/lat/model.py:88\u001b[0m, in \u001b[0;36mget_token_loss\u001b[0;34m(model, tokens, mask, is_away)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_loss\u001b[39m(\n\u001b[1;32m     80\u001b[0m     model: LlamaForCausalLM,\n\u001b[1;32m     81\u001b[0m     tokens: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# push towards completions that are correct (or away)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# mask ensures you only look at completions\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     89\u001b[0m     final_logits \u001b[38;5;241m=\u001b[39m logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][mask[:, \u001b[38;5;241m1\u001b[39m:]]\n\u001b[1;32m     90\u001b[0m     labels \u001b[38;5;241m=\u001b[39m tokens[:, \u001b[38;5;241m1\u001b[39m:][mask[:, \u001b[38;5;241m1\u001b[39m:]]\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1000\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    988\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    989\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    990\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m         position_embeddings,\n\u001b[1;32m    998\u001b[0m     )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:744\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    743\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 744\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    746\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:124\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    123\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 124\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    126\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 10.75 MiB is free. Including non-PyTorch memory, this process has 35.60 GiB memory in use. Process 3510749 has 43.47 GiB memory in use. Of the allocated memory 34.59 GiB is allocated by PyTorch, and 378.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "updated_model = model\n",
    "frozen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, revision=\"main\", torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "updated_model = run_rmu(\n",
    "    updated_model,\n",
    "    frozen_model,  # function to make another version of the model for reference embeddings\n",
    "    tokenizer,  # tokenizer\n",
    "    forget_data_list,  # forget data\n",
    "    retain_data_list,  # retain data\n",
    "    use_lat=True,\n",
    "    n_def_per_adv_steps=4,\n",
    "    n_pgd_steps=16,\n",
    "    max_num_batches=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
