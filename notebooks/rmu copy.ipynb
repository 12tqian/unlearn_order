{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_tools.gpu import get_gpus_available\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9048cc67454c659558d1a3906e2409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "updated_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, token=hf_access_token\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "unlearning_task = \"cyber\"\n",
    "\n",
    "forget_corpus = (\n",
    "    \"cyber-forget-corpus\" if unlearning_task == \"cyber\" else \"bio-forget-corpus\"\n",
    ")\n",
    "retain_corpus = \"wikitext\"\n",
    "\n",
    "\n",
    "def get_unlearning_rmu_dataset(name, min_len=0, batch_size=4):\n",
    "    data = []\n",
    "    if name == \"wikitext\":\n",
    "        raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    elif name == \"cyber-forget-corpus\":\n",
    "        raw_data = load_dataset(\"cais/wmdp-corpora\", name=name, split=\"train\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    else:  # wmdp bio must be pre-downloaded\n",
    "        for line in open(f\"data/{name}.jsonl\", \"r\"):\n",
    "            raw_text = json.loads(line)[\"text\"]\n",
    "            if len(raw_text) > min_len:\n",
    "                data.append(str(raw_text))\n",
    "    return data\n",
    "\n",
    "\n",
    "def _get_unlearning_rmu_dataset(name, min_len=0, batch_size=4):\n",
    "    data = []\n",
    "    if name == \"wikitext\":\n",
    "        raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    elif name == \"cyber-forget-corpus\":\n",
    "        raw_data = load_dataset(\"cais/wmdp-corpora\", name=name, split=\"train\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    else:  # wmdp bio must be pre-downloaded\n",
    "        for line in open(f\"data/{name}.jsonl\", \"r\"):\n",
    "            raw_text = json.loads(line)[\"text\"]\n",
    "            if len(raw_text) > min_len:\n",
    "                data.append(str(raw_text))\n",
    "    return data\n",
    "\n",
    "\n",
    "forget_list = _get_unlearning_rmu_dataset(forget_corpus)\n",
    "retain_list = _get_unlearning_rmu_dataset(retain_corpus)\n",
    "forget_data_list = get_unlearning_rmu_dataset(forget_corpus)\n",
    "retain_data_list = get_unlearning_rmu_dataset(retain_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n\u001b[0;32m----> 3\u001b[0m dset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforget_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:970\u001b[0m, in \u001b[0;36mDataset.from_list\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03mConvert a list of dicts to a `pyarrow.Table` to create a [`Dataset`]`.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    [`Dataset`]\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# for simplicity and consistency wrt OptimizedTypedSequence we do not use InMemoryTable.from_pylist here\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m mapping \u001b[38;5;241m=\u001b[39m {k: [\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(k) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m mapping] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m mapping[\u001b[38;5;241m0\u001b[39m]} \u001b[38;5;28;01mif\u001b[39;00m mapping \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(mapping, features, info, split)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from unlearn_order.algos.lat.hooks import add_hooks\n",
    "from unlearn_order.algos.lat.model import get_adversary_hook, projected_gradient_descent\n",
    "from unlearn_order.algos.lat.utils import (\n",
    "    get_layer_module,\n",
    "    set_model_grads,\n",
    "    set_params_grads,\n",
    ")\n",
    "from typing import List, Dict\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "def get_layers_params(\n",
    "    model: LlamaForCausalLM, layers: List[int], target_modules: List[str]\n",
    "):\n",
    "    params = []\n",
    "    for layer in layers:\n",
    "        for name, module in model.model.layers[layer].named_modules():\n",
    "            if any([target in name for target in target_modules]):\n",
    "                for param in module.parameters():\n",
    "                    params.append(param)\n",
    "    return params\n",
    "\n",
    "\n",
    "def forward_with_cache(\n",
    "    model: LlamaForCausalLM,\n",
    "    inputs: Dict[str, torch.Tensor],\n",
    "    module: torch.nn.Module,\n",
    "    no_grad: bool = True,\n",
    "):\n",
    "    # define a tensor with the size of our cached activations\n",
    "    cache = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            cache.append(output[0])\n",
    "        else:\n",
    "            cache.append(output)\n",
    "        return None\n",
    "\n",
    "    hook_handle = module.register_forward_hook(hook)\n",
    "\n",
    "    if no_grad:\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    else:\n",
    "        _ = model(**inputs)\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    return cache[0]\n",
    "\n",
    "\n",
    "# you're optimizing layer_ids\n",
    "# there is a layer where you're examining the activations at and trying to make garbage\n",
    "# then you only train on the laeyrs before that, this is just shoving garbage in\n",
    "# not real unlearning?\n",
    "# let us also latently perturb at the same layer\n",
    "\n",
    "\n",
    "# remember params\n",
    "def run_rmu(\n",
    "    updated_model: LlamaForCausalLM,\n",
    "    frozen_model: LlamaForCausalLM,\n",
    "    tokenizer: LlamaTokenizer,\n",
    "    forget_data_list,\n",
    "    retain_data_list,\n",
    "    target_modules: List[str] = [\"down_proj\"],\n",
    "    alpha: float = 1200.0,\n",
    "    layer: int = 8,  # layers to do RMU in\n",
    "    lr_def: float = 5.0e-5,\n",
    "    max_num_batches: int = 200,\n",
    "    steering_coef: float = 6.5,\n",
    "    use_lat: bool = True,\n",
    "    epsilon: float = 2,\n",
    "    lr_adv: float = 5e-2,\n",
    "    n_pgd_steps: int = 16,\n",
    "    loss_coefs: Dict[str, float] = {\"towards\": 1, \"away\": 1},\n",
    "    num_epochs: int = 1,\n",
    "    n_def_per_adv_steps: int = 1,\n",
    "):\n",
    "    adv_layer = layer - 1\n",
    "    def_layers = [layer, layer - 1, layer - 2]\n",
    "\n",
    "    updated_model.train()\n",
    "\n",
    "    params = get_layers_params(updated_model, def_layers, target_modules)\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr_def)\n",
    "\n",
    "    updated_module = get_layer_module(updated_model, layer)\n",
    "    frozen_module = get_layer_module(frozen_model, layer)\n",
    "\n",
    "    control_vectors_list = []\n",
    "    for i in range(len(forget_data_list)):\n",
    "        random_vector = torch.rand(\n",
    "            1,\n",
    "            1,\n",
    "            updated_model.config.hidden_size,\n",
    "            dtype=updated_model.dtype,\n",
    "            device=updated_model.device,\n",
    "        )\n",
    "        control_vec = random_vector / torch.norm(random_vector) * steering_coef\n",
    "        control_vectors_list.append(control_vec)\n",
    "\n",
    "    num_batches = max_num_batches\n",
    "    truncation_side = tokenizer.truncation_side\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.truncation_side = \"right\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx in tqdm(range(num_batches)):\n",
    "            set_params_grads(params, True)\n",
    "\n",
    "            control_vec = control_vectors_list[idx]\n",
    "            unlearn_batch = forget_data_list[idx]\n",
    "            retain_batch = retain_data_list[idx]\n",
    "\n",
    "            max_length = 512 if idx == 0 else 768\n",
    "\n",
    "            unlearn_inputs = tokenizer(\n",
    "                unlearn_batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            retain_inputs = tokenizer(\n",
    "                retain_batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            if use_lat:\n",
    "                adv_labels_mask = torch.zeros_like(\n",
    "                    unlearn_inputs[\"input_ids\"], dtype=bool\n",
    "                )\n",
    "                def_labels_mask = torch.zeros_like(\n",
    "                    retain_inputs[\"input_ids\"], dtype=bool\n",
    "                )\n",
    "\n",
    "                for b, example in enumerate(retain_batch):\n",
    "                    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "                    def_labels_mask[b, :len_example] = True\n",
    "                for b, example in enumerate(unlearn_batch):\n",
    "                    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "                    adv_labels_mask[b, :len_example] = True\n",
    "\n",
    "                prompt_mask = adv_labels_mask\n",
    "                pgd_batch = {\n",
    "                    \"def_tokens\": retain_inputs[\"input_ids\"].to(updated_model.device),\n",
    "                    \"adv_tokens\": unlearn_inputs[\"input_ids\"].to(updated_model.device),\n",
    "                    \"adv_labels_mask\": adv_labels_mask.to(updated_model.device),\n",
    "                    \"def_labels_mask\": def_labels_mask.to(updated_model.device),\n",
    "                    \"prompt_mask\": prompt_mask.to(updated_model.device),\n",
    "                }\n",
    "                adversary = projected_gradient_descent(\n",
    "                    updated_model,\n",
    "                    pgd_batch,\n",
    "                    layer=adv_layer,\n",
    "                    epsilon=epsilon,\n",
    "                    n_pgd_steps=n_pgd_steps,\n",
    "                    lr_adv=lr_adv,\n",
    "                    loss_coefs=loss_coefs,\n",
    "                )\n",
    "\n",
    "                set_model_grads(adversary, False)\n",
    "                set_params_grads(params, True)\n",
    "\n",
    "                for step in range(n_def_per_adv_steps):\n",
    "\n",
    "                    # Unlearning loss\n",
    "                    with add_hooks(\n",
    "                        module_forward_hooks=[\n",
    "                            (\n",
    "                                get_layer_module(updated_model, layer),\n",
    "                                get_adversary_hook(adversary),\n",
    "                            )\n",
    "                        ],\n",
    "                        module_forward_pre_hooks=[],\n",
    "                    ):\n",
    "                        updated_forget_activations = forward_with_cache(\n",
    "                            updated_model,\n",
    "                            unlearn_inputs,\n",
    "                            module=updated_module,\n",
    "                            no_grad=False,\n",
    "                        ).to(updated_model.device)\n",
    "\n",
    "                    unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_forget_activations, control_vec\n",
    "                    )\n",
    "\n",
    "                    # Retain loss\n",
    "                    updated_retain_activations = forward_with_cache(\n",
    "                        updated_model,\n",
    "                        retain_inputs,\n",
    "                        module=updated_module,\n",
    "                        no_grad=False,\n",
    "                    ).to(updated_model.device)\n",
    "                    frozen_retain_activations = forward_with_cache(\n",
    "                        frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                    ).to(updated_model.device)\n",
    "\n",
    "                    retain_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_retain_activations, frozen_retain_activations\n",
    "                    )\n",
    "                    retain_loss *= alpha\n",
    "\n",
    "                    # Update model\n",
    "                    loss = unlearn_loss + retain_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    # Unlearning loss\n",
    "                    updated_forget_activations = forward_with_cache(\n",
    "                        updated_model,\n",
    "                        unlearn_inputs,\n",
    "                        module=updated_module,\n",
    "                        no_grad=False,\n",
    "                    ).to(updated_model.device)\n",
    "                    unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_forget_activations, control_vec\n",
    "                    )\n",
    "\n",
    "                    # Retain loss\n",
    "                    updated_retain_activations = forward_with_cache(\n",
    "                        updated_model,\n",
    "                        retain_inputs,\n",
    "                        module=updated_module,\n",
    "                        no_grad=False,\n",
    "                    ).to(updated_model.device)\n",
    "                    frozen_retain_activations = forward_with_cache(\n",
    "                        frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                    ).to(updated_model.device)\n",
    "\n",
    "                    retain_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_retain_activations, frozen_retain_activations\n",
    "                    )\n",
    "                    retain_loss *= alpha\n",
    "\n",
    "                    # Update model\n",
    "                    loss = unlearn_loss + retain_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Batch: {idx}, Unlearn Loss: {unlearn_loss.item()}, Retain Loss: {retain_loss.item()}, Total Loss: {loss.item()}\"\n",
    "                )\n",
    "\n",
    "    tokenizer.truncation_side = truncation_side\n",
    "\n",
    "    return updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/mnt/align4_drive/data/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/.no_exist/892b3d7a7b1cf10c7a701c60881cd93df615734c/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6606978ee74baaa6f438f17cb33a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_2959119/2126138752.py:193: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 512, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "/tmp/ipykernel_2959119/2126138752.py:223: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 512, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "  0%|          | 1/200 [00:08<28:27,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Unlearn Loss: 0.09716796875, Retain Loss: 0.0576171875, Total Loss: 0.154296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2959119/2126138752.py:193: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 768, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "/tmp/ipykernel_2959119/2126138752.py:223: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 768, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "  6%|▌         | 11/200 [02:15<40:02, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 10, Unlearn Loss: 0.061279296875, Retain Loss: 0.006439208984375, Total Loss: 0.06787109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [04:22<38:03, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Unlearn Loss: 0.06005859375, Retain Loss: 0.0081787109375, Total Loss: 0.068359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 31/200 [06:30<36:01, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 30, Unlearn Loss: 0.059326171875, Retain Loss: 0.01397705078125, Total Loss: 0.0732421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [08:37<33:49, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Unlearn Loss: 0.057373046875, Retain Loss: 0.00482177734375, Total Loss: 0.062255859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 51/200 [10:44<31:40, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 50, Unlearn Loss: 0.057373046875, Retain Loss: 0.007049560546875, Total Loss: 0.064453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 61/200 [12:52<29:33, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Unlearn Loss: 0.057373046875, Retain Loss: 0.00433349609375, Total Loss: 0.061767578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 71/200 [14:59<27:26, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 70, Unlearn Loss: 0.058349609375, Retain Loss: 0.0028076171875, Total Loss: 0.06103515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 77/200 [16:15<26:08, 12.75s/it]"
     ]
    }
   ],
   "source": [
    "frozen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, revision=\"main\", torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "updated_model = run_rmu(\n",
    "    updated_model,\n",
    "    frozen_model,  # function to make another version of the model for reference embeddings\n",
    "    tokenizer,  # tokenizer\n",
    "    forget_data_list,  # forget data\n",
    "    retain_data_list,  # retain data\n",
    "    use_lat=True,\n",
    "    n_def_per_adv_steps=4,\n",
    "    n_pgd_steps=16,\n",
    "    max_num_batches=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Step-by-step instructions for conducting a DDOS cyber attack:\"\n",
    "\n",
    "# input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "# output = updated_model.generate(input_ids, max_length=100)\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0.619963 s\n",
      "File: /mnt/align1_drive/tcqian/unlearning_order/src/unlearn_order/algos/lat/model.py\n",
      "Function: projected_gradient_descent at line 125\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   125                                           def projected_gradient_descent(\n",
      "   126                                               model: LlamaForCausalLM,\n",
      "   127                                               batch: Dict[str, torch.Tensor],\n",
      "   128                                               layer: int,\n",
      "   129                                               attack_completions: bool = False,\n",
      "   130                                               epsilon: float = 20.0,\n",
      "   131                                               n_pgd_steps: int = 16,\n",
      "   132                                               lr_adv: float = 1e-3,\n",
      "   133                                               loss_coefs: Dict[str, float] = {\n",
      "   134                                                   \"towards\": 1.0,\n",
      "   135                                                   \"away\": 1.0,\n",
      "   136                                                   \"kl\": 1.0,\n",
      "   137                                                   \"sft\": 1.0,\n",
      "   138                                               },\n",
      "   139                                           ):\n",
      "   140         1    2479611.0    2e+06      0.4      set_model_grads(model, False)\n",
      "   141                                           \n",
      "   142         1        570.0    570.0      0.0      attack_mask = batch[\"prompt_mask\"]\n",
      "   143                                           \n",
      "   144                                           \n",
      "   145         1        220.0    220.0      0.0      if attack_completions:\n",
      "   146                                                   adv_mask = batch[\"adv_labels_mask\"]\n",
      "   147                                                   def_mask = batch[\"def_labels_mask\"]\n",
      "   148                                                   attack_mask = torch.any(torch.stack([attack_mask, adv_mask, def_mask]), dim=0)\n",
      "   149                                           \n",
      "   150         1     218620.0 218620.0      0.0      adversary = Adversary(model, attack_mask, epsilon)\n",
      "   151                                           \n",
      "   152         1      65080.0  65080.0      0.0      optimizer = torch.optim.Adam(adversary.parameters(), lr=lr_adv)\n",
      "   153                                           \n",
      "   154         2       3280.0   1640.0      0.0      for _ in range(n_pgd_steps):\n",
      "   155         1     160660.0 160660.0      0.0          optimizer.zero_grad()\n",
      "   156                                           \n",
      "   157         3      56040.0  18680.0      0.0          with add_hooks(\n",
      "   158         1        250.0    250.0      0.0              module_forward_hooks=[\n",
      "   159         1      22540.0  22540.0      0.0                  (get_layer_module(model, layer), get_adversary_hook(adversary))\n",
      "   160                                                       ],\n",
      "   161         1        170.0    170.0      0.0              module_forward_pre_hooks=[],\n",
      "   162                                                   ):\n",
      "   163         1  306216785.0    3e+08     49.4              losses = get_towards_away_loss(model, batch)\n",
      "   164                                           \n",
      "   165                                                   # TODO(tcqian): investigate gradient clipping and nans\n",
      "   166         1      76510.0  76510.0      0.0          total_loss = sum(combine_losses(losses, loss_coefs).values())\n",
      "   167         1  309803516.0    3e+08     50.0          total_loss.backward()\n",
      "   168         1     703170.0 703170.0      0.1          optimizer.step()\n",
      "   169                                           \n",
      "   170                                                   # assumption is you always step/project in that order\n",
      "   171         1     154450.0 154450.0      0.0          adversary.project()\n",
      "   172                                           \n",
      "   173         1       1150.0   1150.0      0.0      return adversary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "batch = forget_data_list[idx]\n",
    "\n",
    "unlearn_batch = batch\n",
    "retain_batch = batch\n",
    "\n",
    "max_length = 512 if idx == 0 else 768\n",
    "\n",
    "unlearn_inputs = tokenizer(\n",
    "    unlearn_batch,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    ").to(updated_model.device)\n",
    "\n",
    "retain_inputs = tokenizer(\n",
    "    retain_batch,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    ").to(updated_model.device)\n",
    "\n",
    "adv_layer = 7\n",
    "epsilon = 2\n",
    "\n",
    "adv_labels_mask = torch.zeros_like(unlearn_inputs[\"input_ids\"], dtype=bool)\n",
    "\n",
    "def_labels_mask = torch.zeros_like(retain_inputs[\"input_ids\"], dtype=bool)\n",
    "\n",
    "for b, example in enumerate(retain_batch):\n",
    "    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "    def_labels_mask[b, :len_example] = True\n",
    "\n",
    "for b, example in enumerate(unlearn_batch):\n",
    "    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "    adv_labels_mask[b, :len_example] = True\n",
    "\n",
    "prompt_mask = adv_labels_mask\n",
    "\n",
    "n_pgd_steps = 1\n",
    "lr_adv = 5e-2\n",
    "loss_coefs = {\"towards\": 1, \"away\": 1}\n",
    "\n",
    "\n",
    "pgd_batch = {\n",
    "    \"def_tokens\": retain_inputs[\"input_ids\"].to(updated_model.device),\n",
    "    \"adv_tokens\": unlearn_inputs[\"input_ids\"].to(updated_model.device),\n",
    "    \"adv_labels_mask\": adv_labels_mask.to(updated_model.device),\n",
    "    \"def_labels_mask\": def_labels_mask.to(updated_model.device),\n",
    "    \"prompt_mask\": prompt_mask.to(updated_model.device),\n",
    "}\n",
    "\n",
    "\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "lp = LineProfiler()\n",
    "lp_wrapper = lp(projected_gradient_descent)\n",
    "adversary = lp_wrapper(\n",
    "    updated_model,\n",
    "    pgd_batch,\n",
    "    layer=adv_layer,\n",
    "    epsilon=epsilon,\n",
    "    n_pgd_steps=n_pgd_steps,\n",
    "    lr_adv=lr_adv,\n",
    "    loss_coefs=loss_coefs,\n",
    ")\n",
    "lp.print_stats()\n",
    "\n",
    "\n",
    "# projected_gradient_descent(\n",
    "#             updated_model,\n",
    "#             pgd_batch,\n",
    "#             layer=adv_layer,\n",
    "#             epsilon=epsilon,\n",
    "#             n_pgd_steps=n_pgd_steps,\n",
    "#             lr_adv=lr_adv,\n",
    "#             loss_coefs=loss_coefs,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768, 4096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversary.attack.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
