{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.dataset import load_dataset\n",
    "data_dir = Path(cwd).parent / \"data\" / \"random_bd\"\n",
    "\n",
    "files = [\"split_0.jsonl\"]\n",
    "dataset = load_dataset(data_dir, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1dd55277144d01a76878fedcd54fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# i should prepare context vectors\n",
    "from typing import Dict\n",
    "from transformers import LlamaTokenizer\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from unlearn_order.utils import create_prompt, create_prompt_letter_answer, Point\n",
    "\n",
    "def map_fn(data: Point, tokenizer: LlamaTokenizer):\n",
    "    context = create_prompt(data)\n",
    "    context_masks = []\n",
    "    completion_masks = []\n",
    "    input_ids = []\n",
    "\n",
    "    for idx, choice in enumerate(data[\"choices\"]):\n",
    "        new_data = deepcopy(data)\n",
    "        new_data[\"answer\"] = idx\n",
    "        completion = create_prompt_letter_answer(new_data)\n",
    "        context_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "        input_id = tokenizer.encode(completion, return_tensors=\"pt\")\n",
    "\n",
    "        context_ids = context_ids[0]\n",
    "        input_id = input_id[0]\n",
    "        \n",
    "        all_ids = torch.cat([context_ids, input_id])\n",
    "        context_mask = torch.zeros_like(all_ids)\n",
    "        context_mask[:context_ids.size(0)] = 1\n",
    "\n",
    "        completion_mask = torch.zeros_like(all_ids)\n",
    "        completion_mask[context_ids.size(0):] = 1\n",
    "\n",
    "        context_masks.append(context_mask)\n",
    "        completion_masks.append(completion_mask)\n",
    "        input_ids.append(all_ids)\n",
    "\n",
    "    return {\n",
    "        \"context_masks\": context_masks,\n",
    "        \"completion_masks\": completion_masks,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"answer\": data[\"answer\"]\n",
    "    }\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "dataset = dataset.map(partial(map_fn, tokenizer=tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128009\n"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN_ID = tokenizer.eos_token_id\n",
    "print(PAD_TOKEN_ID)\n",
    "\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "# byte length normalization\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def batch_force(batch: Dict[str, torch.Tensor], max_length: int = 128, pad_token_dict: Dict[str, int] = {\n",
    "    \"completion_masks\": 0,\n",
    "    \"context_masks\": 0,\n",
    "    \"input_ids\": PAD_TOKEN_ID,\n",
    "}):\n",
    "    # first dimension of each tensor is the batch size\n",
    "    # batch[key] = [batch_size, seq_len]\n",
    "    # truncate to max length\n",
    "    keys = list(pad_token_dict.keys())\n",
    "    answers = []\n",
    "    n_choices = []\n",
    "    new_batch = {}\n",
    "    for samples in batch:\n",
    "        n = 0\n",
    "        for key in keys:\n",
    "            if key not in new_batch:\n",
    "                new_batch[key] = []\n",
    "            for idx in range(len(samples[key])):\n",
    "                new_batch[key].append(samples[key][idx][:max_length])\n",
    "            n = len(samples[key])\n",
    "        n_choices.append(n)\n",
    "        answers.append(samples[\"answer\"])\n",
    "    \n",
    "\n",
    "    batch = new_batch\n",
    "    \n",
    "    for key in keys:\n",
    "        for idx in range(len(batch[key])):\n",
    "            batch[key][idx] = batch[key][idx][:max_length]  \n",
    "    \n",
    "    # pad on the left\n",
    "    attention_masks = []\n",
    "    for key in keys:\n",
    "        max_len = max(len(batch[key][idx]) for idx in range(len(batch[key])))\n",
    "        \n",
    "        for idx in range(len(batch[key])):\n",
    "            padding_length = max_len - len(batch[key][idx])\n",
    "            attention_mask = torch.tensor([0] * padding_length + [1] * len(batch[key][idx]))\n",
    "            batch[key][idx] =  torch.cat([torch.tensor([\n",
    "                pad_token_dict[key]\n",
    "            ] * padding_length, dtype=torch.int64), torch.tensor(batch[key][idx])])\n",
    "\n",
    "            # hack because i only want one attention mask for sample, not one per key\n",
    "            if key == keys[0]:\n",
    "                attention_masks.append(attention_mask)\n",
    "    return {\n",
    "        \"context_masks\": torch.stack(batch[\"context_masks\"]),\n",
    "        \"completion_masks\": torch.stack(batch[\"completion_masks\"]),\n",
    "        \"attention_masks\": torch.stack(attention_masks),\n",
    "        \"input_ids\": torch.stack(batch[\"input_ids\"]),\n",
    "        \"answers\": torch.tensor(answers),\n",
    "        \"n_choices\": torch.tensor(n_choices)\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "def collate_fn(batch, max_length=128):\n",
    "    # Get the token ids and find the maximum length in the current batc\n",
    "    return batch_force(batch, max_length=max_length)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_masks torch.Size([16, 73])\n",
      "completion_masks torch.Size([16, 73])\n",
      "attention_masks torch.Size([16, 73])\n",
      "input_ids torch.Size([16, 73])\n",
      "answers torch.Size([4])\n",
      "n_choices torch.Size([4])\n",
      "tensor([128009, 128009, 128000,   4599,    574,  40139,  44707,   9405,   5380,\n",
      "            32,     13,    220,   2550,     20,    198,     33,     13,    220,\n",
      "          1049,     20,    198,     34,     13,    220,   2366,     17,    198,\n",
      "            35,     13,    220,   4468,     22,    198,  16533,     25, 128000,\n",
      "          4599,    574,  40139,  44707,   9405,   5380,     32,     13,    220,\n",
      "          2550,     20,    198,     33,     13,    220,   1049,     20,    198,\n",
      "            34,     13,    220,   2366,     17,    198,     35,     13,    220,\n",
      "          4468,     22,    198,  16533,     25,    423,     13,    220,   4468,\n",
      "            22])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    for k,v in batch.items():\n",
    "        print(k, v.shape)\n",
    "    print(batch[\"input_ids\"][-5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b2aa17b57c46a09f342b4503521b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from research_tools.gpu import get_gpus_available\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n",
      "[88, 91, 91, 90]\n",
      "tensor(1.1299, device='cuda:0', grad_fn=<NllLossBackward0>) tensor(1.1299, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([0.4854, 0.4425, 0.5307, 0.4874], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_logits_loss(logits, labels, completion_masks):\n",
    "    # logits = [batch_size, seq_len, vocab_size]\n",
    "    # labels = [batch_size, seq_len]\n",
    "    # completion_masks = [batch_size, seq_len]\n",
    "    # mask out the padding tokens\n",
    "    # print(labels.shape, logits.shape, completion_masks.shape)\n",
    "    # shift the labels to the right\n",
    "    labels = labels[..., 1:]\n",
    "    logits = logits[..., :-1, :]\n",
    "    batch_size = labels.size(0)\n",
    "    seq_len = labels.size(1)\n",
    "\n",
    "    logits = logits.reshape((batch_size * seq_len, -1))\n",
    "    labels = labels.reshape((batch_size * seq_len))\n",
    "\n",
    "    ce_loss = F.cross_entropy(logits, labels, reduction=\"none\")\n",
    "    ce_loss = ce_loss.view(batch_size, seq_len)\n",
    "    ce_loss = ce_loss * completion_masks[..., 1:]\n",
    "    normalization = completion_masks[..., 1:].sum(axis=1)\n",
    "    ce_loss = ce_loss.sum(axis=1) \n",
    "    \n",
    "    total_tokens = normalization.sum()\n",
    "\n",
    "    loss = ce_loss.sum() / total_tokens\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_byte_normalized_loss(input_ids, logits, labels, completion_masks, tokenizer):\n",
    "    # logits = [batch_size, seq_len, vocab_size]\n",
    "    # labels = [batch_size, seq_len]\n",
    "    # completion_masks = [batch_size, seq_len]\n",
    "    # mask out the padding tokens\n",
    "    # print(labels.shape, logits.shape, completion_masks.shape)\n",
    "    # shift the labels to the right\n",
    "    batch_size = labels.size(0)\n",
    "    byte_lengths = []\n",
    "    \n",
    "    for idx in range(batch_size):\n",
    "        completion = tokenizer.decode(input_ids[idx][completion_masks[idx] == 1])\n",
    "        length = len(completion.encode(\"utf-8\"))\n",
    "        byte_lengths.append(length)\n",
    "    \n",
    "    # get utf-8 byte lengths\n",
    "    labels = labels[..., 1:]\n",
    "    logits = logits[..., :-1, :]\n",
    "\n",
    "    batch_size = labels.size(0)\n",
    "    seq_len = labels.size(1)\n",
    "\n",
    "    logits = logits.reshape((batch_size * seq_len, -1))\n",
    "    labels = labels.reshape((batch_size * seq_len))\n",
    "\n",
    "    ce_loss = F.cross_entropy(logits, labels, reduction=\"none\")\n",
    "    ce_loss = ce_loss.view(batch_size, seq_len)\n",
    "    ce_loss = ce_loss * completion_masks[..., 1:]\n",
    "    ce_loss = ce_loss.sum(axis=1) \n",
    "    ce_loss = ce_loss / torch.tensor(byte_lengths, dtype=torch.float32, device=device)\n",
    "    return ce_loss\n",
    "\n",
    "for batch in dataloader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_masks = batch[\"attention_masks\"].to(device)\n",
    "    completion_masks = batch[\"completion_masks\"].to(device)\n",
    "    answers = batch[\"answers\"].to(device)\n",
    "    n_choices = batch[\"n_choices\"].to(device)\n",
    "\n",
    "    # take the inds for the correct answer choice\n",
    "    take_inds = n_choices.cumsum(axis=0)[:-1]\n",
    "    take_inds = torch.cat([torch.tensor([0], device=device), take_inds])\n",
    "    take_inds = take_inds + answers\n",
    "\n",
    "    input_ids = input_ids[take_inds]\n",
    "    attention_masks = attention_masks[take_inds]\n",
    "    completion_masks = completion_masks[take_inds]\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    labels[completion_masks == 0] = -100\n",
    "    outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    print(\"HI\")\n",
    "    cross_entropy = compute_logits_loss(outputs.logits, labels, completion_masks)\n",
    "    other_loss = compute_byte_normalized_loss(input_ids, outputs.logits, labels, completion_masks, tokenizer)\n",
    "    print(loss, cross_entropy)\n",
    "    print(other_loss)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus loss is just take union of context+completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
