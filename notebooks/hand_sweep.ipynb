{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from research_tools.gpu import get_gpus_available\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "n_gpus = 1\n",
    "\n",
    "gpus_available = get_gpus_available()\n",
    "n_gpus = min(n_gpus, len(gpus_available))\n",
    "gpus = gpus_available[:n_gpus]\n",
    "\n",
    "assert n_gpus > 0, \"No GPUs available\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2145cf03fad4d69a8c416d29be50a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from research_tools.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, token=hf_access_token\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relearn.datasets.utils import (\n",
    "    Datasets,\n",
    ")\n",
    "from relearn.datasets.corpus import process as process_corpus\n",
    "from relearn.datasets.mcq import process as process_mcq\n",
    "import pickle\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "cache_path = data_dir / \"full.pickle\"\n",
    "\n",
    "assert cache_path.exists(), \"Cache file does not exist\"\n",
    "with open(cache_path, \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e09abf10a246a786dff0bed0c60960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3916648/3804635791.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  control_vecs = torch.load(control_vecs_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m12tqian\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/align4_drive/tcqian/unlearn_order/notebooks/wandb/run-20250222_122513-21sqlkvc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/12tqian/relearn/runs/21sqlkvc' target=\"_blank\">neat-haze-4128</a></strong> to <a href='https://wandb.ai/12tqian/relearn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/12tqian/relearn' target=\"_blank\">https://wandb.ai/12tqian/relearn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/12tqian/relearn/runs/21sqlkvc' target=\"_blank\">https://wandb.ai/12tqian/relearn/runs/21sqlkvc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolds.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    106\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(folds, f)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(folds))   \n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_data\u001b[39m(fold_inds: List[\u001b[38;5;28mint\u001b[39m]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from relearn.unlearn.rmu import train_rmu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Dict, List, Optional\n",
    "import itertools\n",
    "import wandb\n",
    "from relearn.evaluate import run_eval\n",
    "import numpy as np\n",
    "from relearn.datasets.folds import get_folds_shuffled, fold_name\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from relearn.unlearn.rmu.utils import get_params\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "config = {\n",
    "    \"model_id\": model_id,\n",
    "    \"k_folds\": 4,\n",
    "    \"lr\": 1e-5,\n",
    "    \"lr_decay\": 1,\n",
    "    \"prefix_forget\": True,\n",
    "    \"sweeping\": True,\n",
    "    \"use_wandb\": True,\n",
    "    \"joint_train\": True,\n",
    "    \"magnitude\": 6.5,\n",
    "    \"forget_alpha\": 1,\n",
    "    \"retain_alpha\": 16,\n",
    "    \"actual_retain_alpha\": 16.0,\n",
    "    \"batch_size\": 4,\n",
    "    \"epsilon\": 1e-6,\n",
    "    \"activation_layer\": 7,\n",
    "    \"train_layers\": [5, 6, 7],\n",
    "    \"param_names\": [\"down_proj\"],\n",
    "    \"epochs_per_fold\": 12,\n",
    "    \"forget_decay\": 0.5,\n",
    "}\n",
    "forget_records_dict = data[Datasets.WMDP]\n",
    "retain_records_dict = data[\"retain\"]\n",
    "\n",
    "forget_decay = config[\"forget_decay\"]\n",
    "k_folds = config[\"k_folds\"]\n",
    "lr = config[\"lr\"]\n",
    "lr_decay = config[\"lr_decay\"]\n",
    "prefix_forget = config[\"prefix_forget\"]\n",
    "sweeping = config[\"sweeping\"]\n",
    "use_wandb = config[\"use_wandb\"]\n",
    "joint_train = config[\"joint_train\"]\n",
    "magnitude = config[\"magnitude\"]\n",
    "forget_alpha = config[\"forget_alpha\"]\n",
    "retain_alpha = config[\"retain_alpha\"]\n",
    "actual_retain_alpha = config[\"actual_retain_alpha\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "epsilon = config[\"epsilon\"]\n",
    "\n",
    "# for now i do prefix forget false\n",
    "\n",
    "activation_layer = config[\"activation_layer\"]\n",
    "train_layers = config[\"train_layers\"]\n",
    "param_names = config[\"param_names\"]\n",
    "epochs_per_fold = config[\"epochs_per_fold\"]\n",
    "\n",
    "\n",
    "cur_i = 1\n",
    "\n",
    "optimizer = AdamW(\n",
    "    get_params(model, train_layers, param_names),\n",
    "    lr=lr,\n",
    ")\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=lr_decay)\n",
    "control_vecs = {}\n",
    "\n",
    "\n",
    "if cur_i > 0:\n",
    "    load_path = Path(f\"../models/two/{cur_i - 1}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        load_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        get_params(model, train_layers, param_names),\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    # optimizer_path = load_path / \"optimizer.pt\"\n",
    "    # optimizer.load_state_dict(torch.load(optimizer_path))\n",
    "\n",
    "    scheduler = ExponentialLR(optimizer, gamma=lr_decay)\n",
    "    # scheduler_path = load_path / \"scheduler.pt\"\n",
    "    # scheduler.load_state_dict(torch.load(scheduler_path))\n",
    "\n",
    "    control_vecs_path = load_path / \"control_vecs.pt\"\n",
    "    control_vecs = torch.load(control_vecs_path)\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"relearn\", config=config, tags=[\"rmu\", \"fold\", \"debug\"], entity=\"12tqian\"\n",
    ")\n",
    "\n",
    "\n",
    "folds = get_folds_shuffled(forget_records_dict, k_folds)\n",
    "\n",
    "\n",
    "print(len(folds))\n",
    "\n",
    "\n",
    "def get_data(fold_inds: List[int]):\n",
    "    if joint_train:\n",
    "        return {fold_name(i): folds[i][\"corpus\"] for i in fold_inds}\n",
    "    else:\n",
    "        return {\n",
    "            fold_name(-1): list(\n",
    "                itertools.chain(\n",
    "                    *[f[i][\"corpus\"] for i, f in enumerate(folds) if i in fold_inds]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "eval_records_dict = {fold_name(i): folds[i][\"val\"] for i in range(k_folds)}\n",
    "eval_records_dict[\"retain\"] = retain_records_dict[\"val\"]\n",
    "\n",
    "base_epoch = 0\n",
    "\n",
    "\n",
    "# forget alpha for one fold, two fold\n",
    "for i in [cur_i]:\n",
    "    print(f\"Unlearning fold {fold_name(i)}\")\n",
    "\n",
    "    if prefix_forget:\n",
    "        forget_fold_inds = list(range(i + 1))\n",
    "    else:\n",
    "        forget_fold_inds = [i]\n",
    "\n",
    "    retain_fold_inds = list(range(i + 1, k_folds))\n",
    "\n",
    "    cur_lr = lr\n",
    "\n",
    "    forget_dict = get_data(forget_fold_inds)\n",
    "    retain_dict = get_data(retain_fold_inds)\n",
    "    retain_dict[\"retain\"] = retain_records_dict[\"corpus\"]\n",
    "\n",
    "    shared_forget_alpha = forget_alpha / (1 + epsilon)\n",
    "    shared_retain_alpha = retain_alpha / (k_folds - 1 + epsilon)\n",
    "\n",
    "    # weird retain coef cause i finetuned for 2/2\n",
    "    model, control_vecs_next, eval_dict = train_rmu(\n",
    "        model,\n",
    "        forget_dict,\n",
    "        retain_dict,\n",
    "        eval_records_dict,\n",
    "        magnitude=magnitude,\n",
    "        # if current, then do shared full forget alpha\n",
    "        # all others are split k - 1\n",
    "        # for the retains, you do the thing\n",
    "        # this is so we can share optimizer state? cope\n",
    "        # forget_alphas={\n",
    "        #     k: shared_forget_alpha if idx == i else shared_retain_alpha\n",
    "        #     for idx, k in enumerate(forget_dict.keys())\n",
    "        # },\n",
    "        forget_alphas={\n",
    "            k: (\n",
    "                (forget_alpha / (i + 1 + epsilon)) * forget_decay ** (i - idx)\n",
    "                if prefix_forget\n",
    "                else forget_alpha / (1 + epsilon)\n",
    "            )\n",
    "            for idx, k in enumerate(forget_dict.keys())\n",
    "        },\n",
    "        # retain_alphas={\n",
    "        #     **{k: shared_retain_alpha for idx, k in enumerate(retain_dict.keys())},\n",
    "        #     **{\n",
    "        #         \"retain\": actual_retain_alpha\n",
    "        #         + (retain_alpha if i == k_folds - 1 else 0)\n",
    "        #     },\n",
    "        # },\n",
    "        retain_alphas={\n",
    "            **{\n",
    "                k: retain_alpha / (k_folds - i - 1 + epsilon)\n",
    "                for k in retain_dict.keys()\n",
    "            },\n",
    "            **{\n",
    "                # unsure if below is necessary?\n",
    "                \"retain\": actual_retain_alpha\n",
    "                + (retain_alpha if i == k_folds - 1 else 0)\n",
    "            },\n",
    "        },\n",
    "        lr=cur_lr,\n",
    "        tokenizer=tokenizer,\n",
    "        use_wandb=use_wandb,\n",
    "        eval_at_start=True,\n",
    "        n_epochs=epochs_per_fold,\n",
    "        batch_size=batch_size,\n",
    "        max_batches=None,\n",
    "        base_epoch=base_epoch,\n",
    "        return_control_vecs=True,\n",
    "        control_vecs_init=control_vecs,\n",
    "        print_evals=True,\n",
    "        monitor_name=f\"{fold_name(i)}/acc\",\n",
    "        activation_layer=activation_layer,\n",
    "        train_layers=train_layers,\n",
    "        param_names=param_names,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "\n",
    "    control_vecs.update(control_vecs_next)\n",
    "    base_epoch += epochs_per_fold\n",
    "if sweeping:\n",
    "    full_eval = {\n",
    "        \"forget\": forget_records_dict[\"val\"],\n",
    "        \"retain\": retain_records_dict[\"val\"],\n",
    "    }\n",
    "    res = run_eval(model, tokenizer, full_eval, -1)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.log(res)\n",
    "\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"../models/two/{cur_i}\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(path)\n",
    "\n",
    "import json\n",
    "\n",
    "# save config for later\n",
    "config_path = path / \"my_config.json\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "\n",
    "# save control vecs, map of dict of tensors\n",
    "control_vecs_path = path / \"control_vecs.pt\"\n",
    "torch.save(control_vecs, control_vecs_path)\n",
    "\n",
    "# save optimizer state\n",
    "optimizer_path = path / \"optimizer.pt\"\n",
    "torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "# # save scheduler state\n",
    "# scheduler_path = path / \"scheduler.pt\"\n",
    "# torch.save(scheduler.state_dict(), scheduler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
