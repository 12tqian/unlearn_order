{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wikitext_dset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from research_tools.gpu import get_gpus_available\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da66b88b8b94919aa15e2414a4172cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"google/gemma-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, token=hf_access_token\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearn_order.dataset import load_dataset\n",
    "from unlearn_order.eval import eval_dataset\n",
    "\n",
    "data_dir = Path(\"../data/wmdp-deduped\")\n",
    "n_train = 4\n",
    "n_val = 1\n",
    "batch_size = 1\n",
    "train_files = [f\"corpus_split_{i}.jsonl\" for i in range(n_train)]\n",
    "val_files = [f\"corpus_split_{i}.jsonl\" for i in range(n_train, n_train + n_val)]\n",
    "train_dataset = load_dataset(data_dir, train_files)\n",
    "val_dataset = load_dataset(data_dir, val_files)\n",
    "\n",
    "\n",
    "# acc = eval_dataset(model, tokenizer, train_dataset, batch_size=batch_size)\n",
    "# print(acc)\n",
    "# acc = eval_dataset(model, tokenizer, val_dataset, batch_size=batch_size)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8395650ceb054e0da7a2ff9853b56415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unlearn_order.utils import create_prompt_letter_answer\n",
    "from unlearn_order.dataset import format_single\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "\n",
    "def split_map_func(x):\n",
    "    res = format_single(x, tokenizer)\n",
    "    return {\n",
    "        \"input_ids\": res[\"input_ids\"][:max_length],\n",
    "        \"attention_mask\": res[\"attention_mask\"][:max_length],\n",
    "        \"labels\": res[\"labels\"][:max_length],\n",
    "    }\n",
    "\n",
    "\n",
    "def corpus_map_func(x):\n",
    "    text = x[\"text\"]\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": inputs[\"input_ids\"].clone().squeeze(),\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    corpus_map_func,\n",
    "    batched=False,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    # num_proc=8,\n",
    ")\n",
    "\n",
    "keep_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_dataset.set_format(type=\"torch\", columns=keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[\"attention_mask\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsplit = wikitext_dset[\"train\"]\n",
    "tsplit[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def test_collate(batches):\n",
    "    max_length = max([len(tokenizer.encode(batch[\"text\"])) for batch in batches])\n",
    "\n",
    "    def format_single(batch, max_length):\n",
    "        encoded = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "        prompt_mask = torch.zeros_like(attention_mask)\n",
    "        labels = encoded[\"input_ids\"].squeeze(0)\n",
    "        labels[attention_mask == 0] = -100\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"prompt_mask\": prompt_mask,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    batches = [format_single(batch, max_length) for batch in batches]\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([batch[\"input_ids\"] for batch in batches]),\n",
    "        \"prompt_mask\": torch.stack([batch[\"prompt_mask\"] for batch in batches]),\n",
    "        \"attention_mask\": torch.stack([batch[\"attention_mask\"] for batch in batches]),\n",
    "        \"labels\": torch.stack([batch[\"labels\"] for batch in batches]),\n",
    "    }\n",
    "\n",
    "\n",
    "dl = DataLoader(tsplit, batch_size=4, collate_fn=test_collate, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . \\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = wikitext_dset[\"train\"]\n",
    "min_length = 0\n",
    "dset = dset.filter(lambda x: len(x[\"text\"]) > min_length)\n",
    "dset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 20 forget loss: 0.2909162640571594 retain loss: 2.6840357780456543\n",
      "tensor(8, device='cuda:0') HI THERE torch.Size([1, 49]) torch.Size([1, 49])\n",
      "Epoch 0 step 40 forget loss: 0.022348497062921524 retain loss: 2.8980114459991455\n",
      "tensor(9, device='cuda:0') HI THERE torch.Size([1, 93]) torch.Size([1, 93])\n",
      "Epoch 0 step 60 forget loss: -0.0 retain loss: 3.06994891166687\n",
      "tensor(8, device='cuda:0') HI THERE torch.Size([1, 89]) torch.Size([1, 89])\n",
      "Epoch 0 step 80 forget loss: -0.0 retain loss: 3.2931253910064697\n",
      "tensor(11, device='cuda:0') HI THERE torch.Size([1, 94]) torch.Size([1, 94])\n",
      "Epoch 0 step 100 forget loss: -0.0 retain loss: 2.480302333831787\n",
      "tensor(9, device='cuda:0') HI THERE torch.Size([1, 66]) torch.Size([1, 66])\n",
      "Epoch 0 step 120 forget loss: -0.0 retain loss: 2.572843313217163\n",
      "tensor(32, device='cuda:0') HI THERE torch.Size([1, 179]) torch.Size([1, 179])\n",
      "Epoch 0 step 140 forget loss: -0.0 retain loss: 1.9374988079071045\n",
      "tensor(30, device='cuda:0') HI THERE torch.Size([1, 155]) torch.Size([1, 155])\n",
      "Epoch 0 step 160 forget loss: -0.0 retain loss: 2.514523983001709\n",
      "tensor(4, device='cuda:0') HI THERE torch.Size([1, 38]) torch.Size([1, 38])\n",
      "Epoch 0 step 180 forget loss: -0.0 retain loss: 2.42079496383667\n",
      "tensor(4, device='cuda:0') HI THERE torch.Size([1, 43]) torch.Size([1, 43])\n",
      "Epoch 0 step 200 forget loss: -0.0 retain loss: 2.25307297706604\n",
      "tensor(9, device='cuda:0') HI THERE torch.Size([1, 74]) torch.Size([1, 74])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munlearn_order\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collate_batch\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m----> 8\u001b[0m model, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_ascent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcollate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_collate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model, _, _ = finetune_model_multiple_datasets(\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     tokenizer,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# model, _, _ = finetune_model(model, tokenizer, train_dataset, batch_size, max_epochs=6, eval_every=3, lr=3.2e-6, grad_accum_steps=4)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/src/unlearn_order/finetune.py:130\u001b[0m, in \u001b[0;36mgradient_ascent\u001b[0;34m(model, tokenizer, forget_dataset, forget_collate_fn, retain_dataset, retain_collate_fn, forget_coef, max_epochs, eval_every, lr, batch_size, grad_accum_steps)\u001b[0m\n\u001b[1;32m    127\u001b[0m loss \u001b[38;5;241m=\u001b[39m retain_loss \u001b[38;5;241m+\u001b[39m forget_coef \u001b[38;5;241m*\u001b[39m forget_loss\n\u001b[1;32m    128\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m grad_accum_steps\n\u001b[0;32m--> 130\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m grad_accum_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    132\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# first we want to finetune on the dataset\n",
    "\n",
    "from unlearn_order.finetune import (\n",
    "    finetune_model,\n",
    "    finetune_model_multiple_datasets,\n",
    "    gradient_ascent,\n",
    ")\n",
    "from unlearn_order.dataset import collate_batch\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "model, _, _ = gradient_ascent(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_dataset,\n",
    "    partial(collate_batch, tokenizer=tokenizer),\n",
    "    dset,\n",
    "    test_collate,\n",
    "    10,\n",
    "    grad_accum_steps=4,\n",
    "    batch_size=1,\n",
    ")\n",
    "# model, _, _ = finetune_model_multiple_datasets(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     {\n",
    "#         \"train\": train_dataset,\n",
    "#         \"val\": dset,\n",
    "#     },\n",
    "#     loss_coefs={\n",
    "#         \"train\": -1.0,\n",
    "#         \"val\": 10.0\n",
    "#     },\n",
    "#     batch_size=1,\n",
    "#     grad_accum_steps=4,\n",
    "#     shuffle_labels={\n",
    "#         \"train\": False,\n",
    "#         \"val\": False\n",
    "#     },\n",
    "#     max_epochs=6,\n",
    "#     eval_every=1,\n",
    "#     collate_fns={\n",
    "#         \"train\": partial(\n",
    "#             collate_batch, tokenizer=tokenizer\n",
    "#         ),\n",
    "#         \"val\": test_collate\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# with profile(activities=[ProfilerActivity.CPU],\n",
    "#         profile_memory=True, record_shapes=True) as prof:\n",
    "#     model(inputs)\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
    "# model, _, _ = finetune_model(model, tokenizer, train_dataset, batch_size, max_epochs=6, eval_every=3, lr=3.2e-6, grad_accum_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about spiders: 2006 \n",
      " = = = \n",
      " = \n",
      " 2006 = = \n",
      " = = \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " = = \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " = \n",
      " \n",
      " \n",
      " = \n",
      " \n",
      " \n",
      " \n",
      " = \n",
      " \n",
      " \n",
      " = \n",
      " \n",
      " = \n",
      " = \n",
      " \n",
      " = \n",
      " \n",
      " \n",
      " = \n",
      " \n",
      " \n",
      " \n",
      " = = = \n",
      " \n",
      " = \n",
      " = \n",
      " \n",
      " = \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " = = \n",
      " \n",
      " \n",
      " \n",
      " = = = =\n"
     ]
    }
   ],
   "source": [
    "text = \"Tell me about spiders: \"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_length=100, num_return_sequences=1)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearn_train_files = [f\"corpus_split_{i}.jsonl\" for i in range(n_train)]\n",
    "unlearn_corpus = load_dataset(data_dir, unlearn_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = unlearn_corpus[\"text\"]\n",
    "data_list = [\n",
    "    data_list[i : i + batch_size] for i in range(0, len(data_list), batch_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "unlearning_task = \"cyber\"\n",
    "\n",
    "forget_corpus = (\n",
    "    \"cyber-forget-corpus\" if unlearning_task == \"cyber\" else \"bio-forget-corpus\"\n",
    ")\n",
    "retain_corpus = \"wikitext\"\n",
    "\n",
    "\n",
    "def get_unlearning_rmu_dataset(name, min_len=0, batch_size=4):\n",
    "    data = []\n",
    "    if name == \"wikitext\":\n",
    "        raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    elif name == \"cyber-forget-corpus\":\n",
    "        raw_data = load_dataset(\"cais/wmdp-corpora\", name=name, split=\"train\")\n",
    "        for x in raw_data:\n",
    "            if len(x[\"text\"]) > min_len:\n",
    "                data.append(str(x[\"text\"]))\n",
    "    else:  # wmdp bio must be pre-downloaded\n",
    "        for line in open(f\"data/{name}.jsonl\", \"r\"):\n",
    "            raw_text = json.loads(line)[\"text\"]\n",
    "            if len(raw_text) > min_len:\n",
    "                data.append(str(raw_text))\n",
    "    data = [data[i : i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    return data\n",
    "\n",
    "\n",
    "forget_data_list = data_list\n",
    "retain_data_list = get_unlearning_rmu_dataset(retain_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from unlearn_order.algos.lat.hooks import add_hooks\n",
    "from unlearn_order.algos.lat.model import get_adversary_hook, projected_gradient_descent\n",
    "from unlearn_order.algos.lat.utils import (\n",
    "    get_layer_module,\n",
    "    set_model_grads,\n",
    "    set_params_grads,\n",
    ")\n",
    "from typing import List, Dict\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "def get_layers_params(\n",
    "    model: LlamaForCausalLM, layers: List[int], target_modules: List[str]\n",
    "):\n",
    "    params = []\n",
    "    for layer in layers:\n",
    "        for name, module in model.model.layers[layer].named_modules():\n",
    "            if any([target in name for target in target_modules]):\n",
    "                for param in module.parameters():\n",
    "                    params.append(param)\n",
    "    return params\n",
    "\n",
    "\n",
    "def forward_with_cache(\n",
    "    model: LlamaForCausalLM,\n",
    "    inputs: Dict[str, torch.Tensor],\n",
    "    module: torch.nn.Module,\n",
    "    no_grad: bool = True,\n",
    "):\n",
    "    # define a tensor with the size of our cached activations\n",
    "    cache = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            cache.append(output[0])\n",
    "        else:\n",
    "            cache.append(output)\n",
    "        return None\n",
    "\n",
    "    hook_handle = module.register_forward_hook(hook)\n",
    "\n",
    "    if no_grad:\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    else:\n",
    "        _ = model(**inputs)\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    return cache[0]\n",
    "\n",
    "\n",
    "# you're optimizing layer_ids\n",
    "# there is a layer where you're examining the activations at and trying to make garbage\n",
    "# then you only train on the laeyrs before that, this is just shoving garbage in\n",
    "# not real unlearning?\n",
    "# let us also latently perturb at the same layer\n",
    "\n",
    "\n",
    "# remember params\n",
    "def run_rmu(\n",
    "    updated_model: LlamaForCausalLM,\n",
    "    frozen_model: LlamaForCausalLM,\n",
    "    tokenizer: LlamaTokenizer,\n",
    "    forget_data_list,\n",
    "    retain_data_list,\n",
    "    target_modules: List[str] = [\"down_proj\"],\n",
    "    alpha: float = 1200.0,\n",
    "    layer: int = 8,  # layers to do RMU in\n",
    "    lr_def: float = 5.0e-5,\n",
    "    max_num_batches: int = 200,\n",
    "    steering_coef: float = 6.5,\n",
    "    use_lat: bool = True,\n",
    "    epsilon: float = 2,\n",
    "    lr_adv: float = 5e-2,\n",
    "    n_pgd_steps: int = 16,\n",
    "    loss_coefs: Dict[str, float] = {\"towards\": 1, \"away\": 1},\n",
    "    num_epochs: int = 1,\n",
    "    n_def_per_adv_steps: int = 1,\n",
    "):\n",
    "    adv_layer = layer - 1\n",
    "    def_layers = [layer, layer - 1, layer - 2]\n",
    "\n",
    "    updated_model.train()\n",
    "\n",
    "    params = get_layers_params(updated_model, def_layers, target_modules)\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr_def)\n",
    "\n",
    "    updated_module = get_layer_module(updated_model, layer)\n",
    "    frozen_module = get_layer_module(frozen_model, layer)\n",
    "\n",
    "    control_vectors_list = []\n",
    "    for i in range(len(forget_data_list)):\n",
    "        random_vector = torch.rand(\n",
    "            1,\n",
    "            1,\n",
    "            updated_model.config.hidden_size,\n",
    "            dtype=updated_model.dtype,\n",
    "            device=updated_model.device,\n",
    "        )\n",
    "        control_vec = random_vector / torch.norm(random_vector) * steering_coef\n",
    "        control_vectors_list.append(control_vec)\n",
    "\n",
    "    num_batches = max_num_batches\n",
    "    truncation_side = tokenizer.truncation_side\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.truncation_side = \"right\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx in tqdm(range(num_batches)):\n",
    "            set_params_grads(params, True)\n",
    "\n",
    "            control_vec = control_vectors_list[idx]\n",
    "            unlearn_batch = forget_data_list[idx]\n",
    "            retain_batch = retain_data_list[idx]\n",
    "\n",
    "            max_length = 512 if idx == 0 else 768\n",
    "\n",
    "            unlearn_inputs = tokenizer(\n",
    "                unlearn_batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            retain_inputs = tokenizer(\n",
    "                retain_batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ).to(updated_model.device)\n",
    "\n",
    "            if use_lat:\n",
    "                adv_labels_mask = torch.zeros_like(\n",
    "                    unlearn_inputs[\"input_ids\"], dtype=bool\n",
    "                )\n",
    "                def_labels_mask = torch.zeros_like(\n",
    "                    retain_inputs[\"input_ids\"], dtype=bool\n",
    "                )\n",
    "\n",
    "                for b, example in enumerate(retain_batch):\n",
    "                    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "                    def_labels_mask[b, :len_example] = True\n",
    "                for b, example in enumerate(unlearn_batch):\n",
    "                    len_example = len(tokenizer(example)[\"input_ids\"])\n",
    "                    adv_labels_mask[b, :len_example] = True\n",
    "\n",
    "                prompt_mask = adv_labels_mask\n",
    "                pgd_batch = {\n",
    "                    \"def_tokens\": retain_inputs[\"input_ids\"].to(updated_model.device),\n",
    "                    \"adv_tokens\": unlearn_inputs[\"input_ids\"].to(updated_model.device),\n",
    "                    \"adv_labels_mask\": adv_labels_mask.to(updated_model.device),\n",
    "                    \"def_labels_mask\": def_labels_mask.to(updated_model.device),\n",
    "                    \"prompt_mask\": prompt_mask.to(updated_model.device),\n",
    "                }\n",
    "                adversary = projected_gradient_descent(\n",
    "                    updated_model,\n",
    "                    pgd_batch,\n",
    "                    layer=adv_layer,\n",
    "                    epsilon=epsilon,\n",
    "                    n_pgd_steps=n_pgd_steps,\n",
    "                    lr_adv=lr_adv,\n",
    "                    loss_coefs=loss_coefs,\n",
    "                )\n",
    "\n",
    "                set_model_grads(adversary, False)\n",
    "                set_params_grads(params, True)\n",
    "\n",
    "                for step in range(n_def_per_adv_steps):\n",
    "\n",
    "                    # Unlearning loss\n",
    "                    with add_hooks(\n",
    "                        module_forward_hooks=[\n",
    "                            (\n",
    "                                get_layer_module(updated_model, layer),\n",
    "                                get_adversary_hook(adversary),\n",
    "                            )\n",
    "                        ],\n",
    "                        module_forward_pre_hooks=[],\n",
    "                    ):\n",
    "                        updated_forget_activations = forward_with_cache(\n",
    "                            updated_model,\n",
    "                            unlearn_inputs,\n",
    "                            module=updated_module,\n",
    "                            no_grad=False,\n",
    "                        ).to(updated_model.device)\n",
    "\n",
    "                    unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_forget_activations, control_vec\n",
    "                    )\n",
    "\n",
    "                    # Retain loss\n",
    "                    updated_retain_activations = forward_with_cache(\n",
    "                        updated_model,\n",
    "                        retain_inputs,\n",
    "                        module=updated_module,\n",
    "                        no_grad=False,\n",
    "                    ).to(updated_model.device)\n",
    "                    frozen_retain_activations = forward_with_cache(\n",
    "                        frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                    ).to(updated_model.device)\n",
    "\n",
    "                    retain_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_retain_activations, frozen_retain_activations\n",
    "                    )\n",
    "                    retain_loss *= alpha\n",
    "\n",
    "                    # Update model\n",
    "                    loss = unlearn_loss + retain_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            else:\n",
    "                # Unlearning loss\n",
    "                updated_forget_activations = forward_with_cache(\n",
    "                    updated_model,\n",
    "                    unlearn_inputs,\n",
    "                    module=updated_module,\n",
    "                    no_grad=False,\n",
    "                ).to(updated_model.device)\n",
    "                unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                    updated_forget_activations, control_vec\n",
    "                )\n",
    "\n",
    "                # Retain loss\n",
    "                updated_retain_activations = forward_with_cache(\n",
    "                    updated_model,\n",
    "                    retain_inputs,\n",
    "                    module=updated_module,\n",
    "                    no_grad=False,\n",
    "                ).to(updated_model.device)\n",
    "                frozen_retain_activations = forward_with_cache(\n",
    "                    frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                ).to(updated_model.device)\n",
    "\n",
    "                retain_loss = torch.nn.functional.mse_loss(\n",
    "                    updated_retain_activations, frozen_retain_activations\n",
    "                )\n",
    "                retain_loss *= alpha\n",
    "\n",
    "                # Update model\n",
    "                loss = unlearn_loss + retain_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Batch: {idx},  Unlearn Loss: {unlearn_loss.item()}, Retain Loss: {retain_loss.item()}, Total Loss: {loss.item()}\"\n",
    "                )\n",
    "\n",
    "    tokenizer.truncation_side = truncation_side\n",
    "\n",
    "    return updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc8837cbe9d437698c80746329666a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                   | 0/200 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "/tmp/ipykernel_3516061/4134084959.py:225: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 512, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "  0%|▊                                                                                                                                                          | 1/200 [00:00<01:44,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0,  Unlearn Loss: 0.08740234375, Retain Loss: 0.0, Total Loss: 0.08740234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3516061/4134084959.py:225: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([4, 768, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "  6%|████████▍                                                                                                                                                 | 11/200 [00:07<02:25,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 10,  Unlearn Loss: 0.06298828125, Retain Loss: 0.091796875, Total Loss: 0.154296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████████▏                                                                                                                                         | 21/200 [00:15<02:18,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20,  Unlearn Loss: 0.06298828125, Retain Loss: 0.026611328125, Total Loss: 0.08984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████████▊                                                                                                                                  | 31/200 [00:22<02:11,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 30,  Unlearn Loss: 0.06298828125, Retain Loss: 0.0211181640625, Total Loss: 0.083984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████████████▌                                                                                                                          | 41/200 [00:30<02:03,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40,  Unlearn Loss: 0.06298828125, Retain Loss: 0.0164794921875, Total Loss: 0.07958984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████████████████████████▎                                                                                                                  | 51/200 [00:37<01:55,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 50,  Unlearn Loss: 0.0625, Retain Loss: 0.00897216796875, Total Loss: 0.0712890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████████▉                                                                                                           | 61/200 [00:45<01:48,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60,  Unlearn Loss: 0.061767578125, Retain Loss: 0.00579833984375, Total Loss: 0.0673828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████████████████████▋                                                                                                   | 71/200 [00:52<01:40,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 70,  Unlearn Loss: 0.061767578125, Retain Loss: 0.005889892578125, Total Loss: 0.06787109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████████▎                                                                                           | 81/200 [01:00<01:32,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 80,  Unlearn Loss: 0.0615234375, Retain Loss: 0.01007080078125, Total Loss: 0.07177734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████████████████████████████████████                                                                                    | 91/200 [01:07<01:24,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 90,  Unlearn Loss: 0.06103515625, Retain Loss: 0.0191650390625, Total Loss: 0.080078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████▎                                                                           | 101/200 [01:15<01:16,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 100,  Unlearn Loss: 0.061279296875, Retain Loss: 0.0078125, Total Loss: 0.0693359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████████████████████████████████████████████████▉                                                                    | 111/200 [01:22<01:09,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 110,  Unlearn Loss: 0.06005859375, Retain Loss: 0.0198974609375, Total Loss: 0.080078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████▌                                                            | 121/200 [01:30<01:01,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 120,  Unlearn Loss: 0.06005859375, Retain Loss: 0.00848388671875, Total Loss: 0.068359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 131/200 [01:37<00:53,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 130,  Unlearn Loss: 0.0595703125, Retain Loss: 0.015380859375, Total Loss: 0.0751953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                             | 141/200 [01:45<00:45,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 140,  Unlearn Loss: 0.059326171875, Retain Loss: 0.00860595703125, Total Loss: 0.06787109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 151/200 [01:52<00:38,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 150,  Unlearn Loss: 0.05908203125, Retain Loss: 0.014404296875, Total Loss: 0.0732421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                             | 161/200 [02:00<00:30,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 160,  Unlearn Loss: 0.057861328125, Retain Loss: 0.0152587890625, Total Loss: 0.0732421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 171/200 [02:08<00:22,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 170,  Unlearn Loss: 0.057861328125, Retain Loss: 0.02294921875, Total Loss: 0.0810546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 181/200 [02:15<00:14,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 180,  Unlearn Loss: 0.057373046875, Retain Loss: 0.0185546875, Total Loss: 0.076171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 191/200 [02:23<00:06,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 190,  Unlearn Loss: 0.056640625, Retain Loss: 0.0091552734375, Total Loss: 0.06591796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [02:29<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "updated_model = model\n",
    "frozen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, revision=\"main\", torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "updated_model = run_rmu(\n",
    "    updated_model,\n",
    "    frozen_model,  # function to make another version of the model for reference embeddings\n",
    "    tokenizer,  # tokenizer\n",
    "    forget_data_list,  # forget data\n",
    "    retain_data_list,  # retain data\n",
    "    use_lat=False,\n",
    "    n_def_per_adv_steps=4,\n",
    "    n_pgd_steps=16,\n",
    "    max_num_batches=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6033755274261603\n"
     ]
    }
   ],
   "source": [
    "acc = eval_dataset(updated_model, tokenizer, train_dataset, batch_size=batch_size)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
